{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "blockquote { background: #AEDE94; }\n",
       "h1 { \n",
       "    padding-top: 25px;\n",
       "    padding-bottom: 25px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "h2 { \n",
       "    padding-top: 10px;\n",
       "    padding-bottom: 10px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "\n",
       "div.exercise {\n",
       "\tbackground-color: #ffcccc;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "span.sub-q {\n",
       "\tfont-weight: bold;\n",
       "}\n",
       "div.theme {\n",
       "\tbackground-color: #DDDDDD;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 18pt;\n",
       "}\n",
       "div.gc { \n",
       "\tbackground-color: #AEDE94;\n",
       "\tborder-color: #E9967A; \t \n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 12pt;\n",
       "}\n",
       "p.q1 { \n",
       "    padding-top: 5px;\n",
       "    padding-bottom: 5px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "header {\n",
       "   padding-top: 35px;\n",
       "    padding-bottom: 35px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RUN THIS CELL FOR FORMAT\n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, RidgeCV, LassoCV, ElasticNetCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'> <b> Question 1 [20 pts] </b> </div>\n",
    "\n",
    "Suppose we want to conduct PCA on the model matrix $X \\in \\Re^{n√óp}$, where the columns have been suitably set to zero mean. In this question, we consider the squared reconstruction error:\n",
    "\n",
    "$$  \\parallel XQ- XQ_m \\parallel ^2 $$\n",
    "\n",
    "for a suitable set of eigenvectors forming the matrix $Q_m$, as discussed below. Suppose that we conduct eigendecomposition of $X^T X$ and obtain eigenvalues $\\lambda_1, \\ldots , \\lambda_p$ and principal components $Q$, i.e.\n",
    "\n",
    "$$ X^T X = Q \\Lambda Q ^T $$\n",
    "\n",
    "**1.1** Suppose that the matrix norm is simply the squared dot product, namely\n",
    "\n",
    "$$ \\parallel A \\parallel ^2 = A^T A $$\n",
    "\n",
    "Then, express the reconstruction error as a sum of matrix products.\n",
    "\n",
    "**1.2**  Simplify your result from 5.1 based on properties of the matrices $Q$.\n",
    "\n",
    "**1.3** Now let $Q_m$ be the matrix of the first $m < p$ eigenvectors, namely\n",
    "\n",
    "$$ Q_m = (q_1, \\ldots, q_m, 0, \\ldots, 0) \\in \\Re^{p \\times p} $$\n",
    "\n",
    "Thus, $X Q_m$ is the PCA projection of the data into the space spanned by the first $m$ principal components. Express the products $Q^T_m Q$ and $Q^T Q_m$, again using properties of the eigenbasis $q_1, \\ldots, q_p$.\n",
    "\n",
    "**1.4**  Use your results from 5.3 to finally fully simplify your expression from 5.2.\n",
    "\n",
    "**1.5** Note that the result you obtain should still be a matrix, i.e. this does not define a proper norm on the space of matrices (since the value should be a scalar). Consequently, the true matrix norm is actually the trace of the\n",
    "above result, namely\n",
    "$$ \\parallel A \\parallel ^2  = {\\rm trace} (A^T A) $$\n",
    "Use your result from 5.4 and this new definition to find a simple expression\n",
    "for the reconstruction error in terms of the eigenvalues.\n",
    "\n",
    "**1.6** Interpret your result from (5). In light of your results, does our procedure for PCA (selecting the $m$ substantially larger eigenvalues) make sense? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>**1.1** Suppose that the matrix norm is simply the squared dot product, namely\n",
    "\n",
    "$$ \\parallel A \\parallel ^2 = A^T A $$\n",
    "\n",
    "Then, express the reconstruction error as a sum of matrix products.\n",
    "\n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose $ \\parallel A \\parallel ^2 = A^T A $.\n",
    "\n",
    "Then $ \\parallel XQ- XQ_m \\parallel ^2 = (XQ- XQ_m)^T(XQ- XQ_m)$\n",
    "\n",
    "$= ((XQ)^T - (XQ_m)^T)(XQ- XQ_m)$\n",
    "\n",
    "$= (XQ)^TXQ- (XQ)^TXQ_m - (XQ_m)^TXQ + (XQ_m)^TXQ_m$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>**1.2**  Simplify your result from 5.1 based on properties of the matrices $Q$.\n",
    "\n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have $\\parallel XQ- XQ_m \\parallel ^2 \\;= (XQ)^TXQ- (XQ)^TXQ_m - (XQ_m)^TXQ + (XQ_m)^TXQ_m$\n",
    "\n",
    "$= Q^TX^TXQ- Q^TX^TXQ_m - Q_m^TX^TXQ + Q_m^TX^TXQ_m$\n",
    "\n",
    "$= Q^T(X^TXQ- X^TXQ_m) + Q_m^T(X^TXQ - X^TXQ_m)$\n",
    "\n",
    "$= (Q^T - Q_m^T)(X^TXQ - X^TXQ_m)$\n",
    "\n",
    "$= (Q^T - Q_m^T)(X^TX)(Q - Q_m)$\n",
    "\n",
    "$= (Q^T - Q_m^T)(Q\\Lambda Q^T)(Q - Q_m)$\n",
    "\n",
    "$= (Q^TQ - Q_m^TQ)\\Lambda(Q^TQ - Q^TQ_m)$\n",
    "\n",
    "$= (I - Q_m^TQ)\\Lambda(I - Q^TQ_m)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>**1.3** Now let $Q_m$ be the matrix of the first $m < p$ eigenvectors, namely\n",
    "\n",
    "$$ Q_m = (q_1, \\ldots, q_m, 0, \\ldots, 0) \\in \\Re^{p \\times p} $$\n",
    "\n",
    "Thus, $X Q_m$ is the PCA projection of the data into the space spanned by the first $m$ principal components. Express the products $Q^T_m Q$ and $Q^T Q_m$, again using properties of the eigenbasis $q_1, \\ldots, q_p$.\n",
    "\n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know $ Q_m = (q_1, \\ldots, q_m, 0, \\ldots, 0) \\in \\Re^{p \\times p} $ and $ Q = (q_1, \\ldots, q_p) \\in \\Re^{p \\times p} $\n",
    "\n",
    "Because Q is orthonormal, we know that the following properties hold on the eigenbasis:\n",
    "\n",
    "$ q_i^Tq_j = \\delta_{ij} $\n",
    "\n",
    "$ q_i^Tq_i = \\;\\parallel q_i\\parallel^2\\;= 1 $\n",
    "\n",
    "Thus $ Q_m^TQ = (q_1^Tq1, \\ldots, q_m^Tq_m, 0, \\ldots, 0) \\in \\Re^{p \\times p} $\n",
    "\n",
    "$ = (1, \\ldots, 1, 0, \\ldots, 0) \\in \\Re^{p \\times p} $\n",
    "\n",
    "And $ Q^TQ_m = (q_1^Tq1, \\ldots, q_m^Tq_m, 0, \\ldots, 0) \\in \\Re^{p \\times p} $\n",
    "\n",
    "$ = (1, \\ldots, 1, 0, \\ldots, 0) \\in \\Re^{p \\times p} $\n",
    "\n",
    "So $ Q^TQ_m =  Q_m^TQ = (1, \\ldots, 1, 0, \\ldots, 0) \\in \\Re^{p \\times p} $\n",
    "\n",
    "Essentially, this is a partial identity matrix with $Q_{ij} = 1$ for $(i = j) < m$ and $0$ otherwise.\n",
    "\n",
    "We can denote this partial identity matrix as $I_m$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>**1.4**  Use your results from 5.3 to finally fully simplify your expression from 5.2.\n",
    "\n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\parallel XQ- XQ_m \\parallel^2\\;= (I - Q_m^TQ)\\Lambda(I - Q^TQ_m)$\n",
    "\n",
    "$\\parallel XQ- XQ_m \\parallel^2\\;= (I - I_m)\\Lambda(I - I_m)$\n",
    "\n",
    "$\\parallel XQ- XQ_m \\parallel^2\\;= (0, \\ldots, 0, \\lambda_{m+1}, \\ldots, \\lambda_{p}) \\in \\Re^{p \\times p} $\n",
    "\n",
    "This is the spectrum matrix with only the $(m+1)^{\\rm th}$ through $p^{\\rm th}$ values filled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>**1.5** Note that the result you obtain should still be a matrix, i.e. this does not define a proper norm on the space of matrices (since the value should be a scalar). Consequently, the true matrix norm is actually the trace of the\n",
    "above result, namely\n",
    "$$ \\parallel A \\parallel ^2  = {\\rm trace} (A^T A) $$\n",
    "Use your result from 5.4 and this new definition to find a simple expression\n",
    "for the reconstruction error in terms of the eigenvalues.\n",
    "\n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the trace of the above result,\n",
    "\n",
    "$\\parallel XQ- XQ_m \\parallel^2\\;= {\\rm trace}((0, \\ldots, 0, \\lambda_{m+1}, \\ldots, \\lambda_{p}) \\in \\Re^{p \\times p})$\n",
    "\n",
    "$\\parallel XQ- XQ_m \\parallel^2\\;= \\sum_{i=m+1}^{p}{\\lambda_i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>**1.6** Interpret your result from (5). In light of your results, does our procedure for PCA (selecting the $m$ substantially larger eigenvalues) make sense? Why or why not?</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, in that we achieve the minimum possible squared reconstruction error subject to a threshold of $m$ eigenvalues, because we have ordered them from greatest to least and removed the $m$ largest. Alternatively, these $m$ eigenvalues represent the most parsimonious way to achieve a reconstruction error of at most $\\sum_{i=m+1}^{p}{\\lambda_i}$: although there may be other candidate solutions, all of them must require $m$ or more of the eigenvalues to achieve.\n",
    "\n",
    "Of course, the problems inherent to any response-agnostic dimensionality reduction technique such as PCA are still present. We are reconstructing the data as accurately as we can, but PCA is especially sensitive to noise (or even intentional disruption)! For instance, whereas LASSO would throw out an uncorrelated predictor, PCA could allow it to dominate the representation if the variance were sufficiently high.\n",
    "\n",
    "Thus, PCA should yield excellent results if our data is all high-quality and relevant, but it is important to think about whether our underlying assumptions hold on any particular data set in question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
