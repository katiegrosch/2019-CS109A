{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS109A Introduction to Data Science\n",
    "\n",
    "## Homework 5: $k$-NN Classification, ROC, and PCA\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2019**<br/>\n",
    "**Instructors**: Pavlos Protopapas, Kevin Rader, and Chris Tanner\n",
    "\n",
    "<hr style=\"height:2pt\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "blockquote { background: #AEDE94; }\n",
       "h1 { \n",
       "    padding-top: 25px;\n",
       "    padding-bottom: 25px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "h2 { \n",
       "    padding-top: 10px;\n",
       "    padding-bottom: 10px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "\n",
       "div.exercise {\n",
       "\tbackground-color: #ffcccc;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "span.sub-q {\n",
       "\tfont-weight: bold;\n",
       "}\n",
       "div.theme {\n",
       "\tbackground-color: #DDDDDD;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 18pt;\n",
       "}\n",
       "div.gc { \n",
       "\tbackground-color: #AEDE94;\n",
       "\tborder-color: #E9967A; \t \n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 12pt;\n",
       "}\n",
       "p.q1 { \n",
       "    padding-top: 5px;\n",
       "    padding-bottom: 5px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "header {\n",
       "   padding-top: 35px;\n",
       "    padding-bottom: 35px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RUN THIS CELL \n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INSTRUCTIONS\n",
    "\n",
    "- To submit your assignment follow the instructions given in Canvas.\n",
    "- Restart the kernel and run the whole notebook again before you submit. \n",
    "- As much as possible, try and stick to the hints and functions we import at the top of the homework, as those are the ideas and tools the class supports and is aiming to teach. And if a problem specifies a particular library you're required to use that library, and possibly others from the import list.\n",
    "- Please use .head() when viewing data. Do not submit a notebook that is excessively long because output was not suppressed or otherwise limited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import ttest_ind\n",
    "pd.set_option('display.max_columns', 25)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import metrics\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.api import OLS\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Continuing Cancer Classification from Gene Expressions\n",
    "\n",
    "In this problem, we will continue to work with the 2-class (ALL vs. AML) cancer classification problem from homework 4. The dataset is provided in the file `data/hw4_enhance.csv` and is the same as the previous HW.  Recall: **0 indicates the ALL** class and **1 indicates the AML** class within `Cancer_type`, and columns 2-7130 contain expression levels of 7129 genes recorded from each tissue sample. \n",
    "\n",
    "In the following questions, we will handle the high dimensionality of this problem directly by applying Principal Component Analysis (PCA), and then use the resulting PCA-transformed in logistic regression and $k$-NN modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'><b> Question 1 [20 pts]: Baseline Modeling </b></div>\n",
    "\n",
    "First step is to split the observations into an approximate 80-20 train-test split.  Below is the same code from HW4 to do this for you (we again want to make sure everyone has the same splits). Use the training data set to answer the following questions:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First step is to split the observations into an approximate 80-20 train-test split. Below is some code to do this for you (we want to make sure everyone has the same splits). `Cancer_type` is again our target column.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "df = pd.read_csv('data/hw4_enhance.csv', index_col=0)\n",
    "\n",
    "X_train, X_test, y_train, y_test =train_test_split(df.loc[:, df.columns != 'Cancer_type'], \n",
    "                                                         df.Cancer_type, test_size=0.2, \n",
    "                                                         random_state = 109, \n",
    "                                                         stratify = df.Cancer_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1** Begin by normalizing all predictors just like you did in HW4.  Use these versions of the predictors throughout this assignment.\n",
    "\n",
    "**1.2** Fit an 'unregularized' multiple logistic regression model (set C=100000) with all the gene predictors from the data set (you did this in HW4).  Report the misclassification rate on both the train and test sets.\n",
    "\n",
    "**1.3** Use regularization to improve the predictions from the multiple logistic regression model. Specifically, use LASSO-like regularization and cross-validation to train the model on the training set (you did this in HW4).  Report the classification accuracy on both the train and test sets.\n",
    "\n",
    "**1.4** Use 5-fold cross-validation to select $k$ for a $k$-NN classification model based on the full predictor set.  Choose between `k = 1,2,5,10,20,50,100,200,500`. \n",
    "\n",
    "**1.5** Provide the confusion matrix for all 3 models above and report the false positive and false negative rates (all in the test set).  Briefly interpret what you notice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1 Begin by normalizing all predictors just like you did in HW4.  Use these versions of the predictors throughout this assignment.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AFFX-BioB-5_at</th>\n",
       "      <th>AFFX-BioB-M_at</th>\n",
       "      <th>AFFX-BioB-3_at</th>\n",
       "      <th>AFFX-BioC-5_at</th>\n",
       "      <th>AFFX-BioC-3_at</th>\n",
       "      <th>AFFX-BioDn-5_at</th>\n",
       "      <th>AFFX-BioDn-3_at</th>\n",
       "      <th>AFFX-CreX-5_at</th>\n",
       "      <th>AFFX-CreX-3_at</th>\n",
       "      <th>AFFX-BioB-5_st</th>\n",
       "      <th>AFFX-BioB-M_st</th>\n",
       "      <th>AFFX-BioB-3_st</th>\n",
       "      <th>...</th>\n",
       "      <th>U11863_at</th>\n",
       "      <th>U29175_at</th>\n",
       "      <th>U48730_at</th>\n",
       "      <th>U58516_at</th>\n",
       "      <th>U73738_at</th>\n",
       "      <th>X06956_at</th>\n",
       "      <th>X16699_at</th>\n",
       "      <th>X83863_at</th>\n",
       "      <th>Z17240_at</th>\n",
       "      <th>L49218_f_at</th>\n",
       "      <th>M71243_f_at</th>\n",
       "      <th>Z78285_f_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>601.000000</td>\n",
       "      <td>601.000000</td>\n",
       "      <td>601.000000</td>\n",
       "      <td>601.000000</td>\n",
       "      <td>601.000000</td>\n",
       "      <td>601.000000</td>\n",
       "      <td>601.000000</td>\n",
       "      <td>601.000000</td>\n",
       "      <td>601.000000</td>\n",
       "      <td>601.000000</td>\n",
       "      <td>601.000000</td>\n",
       "      <td>601.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>601.000000</td>\n",
       "      <td>601.000000</td>\n",
       "      <td>601.000000</td>\n",
       "      <td>601.000000</td>\n",
       "      <td>601.000000</td>\n",
       "      <td>601.000000</td>\n",
       "      <td>601.000000</td>\n",
       "      <td>601.000000</td>\n",
       "      <td>601.000000</td>\n",
       "      <td>601.000000</td>\n",
       "      <td>601.000000</td>\n",
       "      <td>601.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.551880</td>\n",
       "      <td>0.631528</td>\n",
       "      <td>0.482168</td>\n",
       "      <td>0.438776</td>\n",
       "      <td>0.618723</td>\n",
       "      <td>0.403753</td>\n",
       "      <td>0.591583</td>\n",
       "      <td>0.544518</td>\n",
       "      <td>0.621268</td>\n",
       "      <td>0.503159</td>\n",
       "      <td>0.592887</td>\n",
       "      <td>0.429101</td>\n",
       "      <td>...</td>\n",
       "      <td>0.537128</td>\n",
       "      <td>0.477589</td>\n",
       "      <td>0.558309</td>\n",
       "      <td>0.486270</td>\n",
       "      <td>0.485819</td>\n",
       "      <td>0.391604</td>\n",
       "      <td>0.590949</td>\n",
       "      <td>0.503939</td>\n",
       "      <td>0.370512</td>\n",
       "      <td>0.498539</td>\n",
       "      <td>0.537380</td>\n",
       "      <td>0.461455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.120852</td>\n",
       "      <td>0.127453</td>\n",
       "      <td>0.149090</td>\n",
       "      <td>0.140154</td>\n",
       "      <td>0.141699</td>\n",
       "      <td>0.153665</td>\n",
       "      <td>0.155006</td>\n",
       "      <td>0.164288</td>\n",
       "      <td>0.128469</td>\n",
       "      <td>0.141734</td>\n",
       "      <td>0.160189</td>\n",
       "      <td>0.153374</td>\n",
       "      <td>...</td>\n",
       "      <td>0.177113</td>\n",
       "      <td>0.147707</td>\n",
       "      <td>0.139064</td>\n",
       "      <td>0.149080</td>\n",
       "      <td>0.164067</td>\n",
       "      <td>0.100772</td>\n",
       "      <td>0.164910</td>\n",
       "      <td>0.138239</td>\n",
       "      <td>0.138930</td>\n",
       "      <td>0.161129</td>\n",
       "      <td>0.154759</td>\n",
       "      <td>0.153690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.476264</td>\n",
       "      <td>0.557454</td>\n",
       "      <td>0.394956</td>\n",
       "      <td>0.352879</td>\n",
       "      <td>0.529837</td>\n",
       "      <td>0.308342</td>\n",
       "      <td>0.500545</td>\n",
       "      <td>0.429288</td>\n",
       "      <td>0.551025</td>\n",
       "      <td>0.418475</td>\n",
       "      <td>0.507330</td>\n",
       "      <td>0.332571</td>\n",
       "      <td>...</td>\n",
       "      <td>0.425708</td>\n",
       "      <td>0.383596</td>\n",
       "      <td>0.479238</td>\n",
       "      <td>0.388388</td>\n",
       "      <td>0.385852</td>\n",
       "      <td>0.334709</td>\n",
       "      <td>0.506111</td>\n",
       "      <td>0.410831</td>\n",
       "      <td>0.280240</td>\n",
       "      <td>0.405717</td>\n",
       "      <td>0.445447</td>\n",
       "      <td>0.356357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.559477</td>\n",
       "      <td>0.641672</td>\n",
       "      <td>0.478321</td>\n",
       "      <td>0.437090</td>\n",
       "      <td>0.631015</td>\n",
       "      <td>0.387398</td>\n",
       "      <td>0.587109</td>\n",
       "      <td>0.557519</td>\n",
       "      <td>0.632589</td>\n",
       "      <td>0.504952</td>\n",
       "      <td>0.611899</td>\n",
       "      <td>0.440909</td>\n",
       "      <td>...</td>\n",
       "      <td>0.524862</td>\n",
       "      <td>0.475118</td>\n",
       "      <td>0.551141</td>\n",
       "      <td>0.478502</td>\n",
       "      <td>0.492022</td>\n",
       "      <td>0.384691</td>\n",
       "      <td>0.614133</td>\n",
       "      <td>0.492102</td>\n",
       "      <td>0.358113</td>\n",
       "      <td>0.496963</td>\n",
       "      <td>0.521019</td>\n",
       "      <td>0.459238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.630576</td>\n",
       "      <td>0.712756</td>\n",
       "      <td>0.554837</td>\n",
       "      <td>0.522697</td>\n",
       "      <td>0.709618</td>\n",
       "      <td>0.493697</td>\n",
       "      <td>0.692712</td>\n",
       "      <td>0.659477</td>\n",
       "      <td>0.706408</td>\n",
       "      <td>0.578614</td>\n",
       "      <td>0.696565</td>\n",
       "      <td>0.535167</td>\n",
       "      <td>...</td>\n",
       "      <td>0.657066</td>\n",
       "      <td>0.570208</td>\n",
       "      <td>0.643646</td>\n",
       "      <td>0.590640</td>\n",
       "      <td>0.584703</td>\n",
       "      <td>0.442659</td>\n",
       "      <td>0.698660</td>\n",
       "      <td>0.583209</td>\n",
       "      <td>0.442772</td>\n",
       "      <td>0.600409</td>\n",
       "      <td>0.633784</td>\n",
       "      <td>0.556818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows √ó 7129 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       AFFX-BioB-5_at  AFFX-BioB-M_at  AFFX-BioB-3_at  AFFX-BioC-5_at  \\\n",
       "count      601.000000      601.000000      601.000000      601.000000   \n",
       "mean         0.551880        0.631528        0.482168        0.438776   \n",
       "std          0.120852        0.127453        0.149090        0.140154   \n",
       "min          0.000000        0.000000        0.000000        0.000000   \n",
       "25%          0.476264        0.557454        0.394956        0.352879   \n",
       "50%          0.559477        0.641672        0.478321        0.437090   \n",
       "75%          0.630576        0.712756        0.554837        0.522697   \n",
       "max          1.000000        1.000000        1.000000        1.000000   \n",
       "\n",
       "       AFFX-BioC-3_at  AFFX-BioDn-5_at  AFFX-BioDn-3_at  AFFX-CreX-5_at  \\\n",
       "count      601.000000       601.000000       601.000000      601.000000   \n",
       "mean         0.618723         0.403753         0.591583        0.544518   \n",
       "std          0.141699         0.153665         0.155006        0.164288   \n",
       "min          0.000000         0.000000         0.000000        0.000000   \n",
       "25%          0.529837         0.308342         0.500545        0.429288   \n",
       "50%          0.631015         0.387398         0.587109        0.557519   \n",
       "75%          0.709618         0.493697         0.692712        0.659477   \n",
       "max          1.000000         1.000000         1.000000        1.000000   \n",
       "\n",
       "       AFFX-CreX-3_at  AFFX-BioB-5_st  AFFX-BioB-M_st  AFFX-BioB-3_st  ...  \\\n",
       "count      601.000000      601.000000      601.000000      601.000000  ...   \n",
       "mean         0.621268        0.503159        0.592887        0.429101  ...   \n",
       "std          0.128469        0.141734        0.160189        0.153374  ...   \n",
       "min          0.000000        0.000000        0.000000        0.000000  ...   \n",
       "25%          0.551025        0.418475        0.507330        0.332571  ...   \n",
       "50%          0.632589        0.504952        0.611899        0.440909  ...   \n",
       "75%          0.706408        0.578614        0.696565        0.535167  ...   \n",
       "max          1.000000        1.000000        1.000000        1.000000  ...   \n",
       "\n",
       "        U11863_at   U29175_at   U48730_at   U58516_at   U73738_at   X06956_at  \\\n",
       "count  601.000000  601.000000  601.000000  601.000000  601.000000  601.000000   \n",
       "mean     0.537128    0.477589    0.558309    0.486270    0.485819    0.391604   \n",
       "std      0.177113    0.147707    0.139064    0.149080    0.164067    0.100772   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.425708    0.383596    0.479238    0.388388    0.385852    0.334709   \n",
       "50%      0.524862    0.475118    0.551141    0.478502    0.492022    0.384691   \n",
       "75%      0.657066    0.570208    0.643646    0.590640    0.584703    0.442659   \n",
       "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "        X16699_at   X83863_at   Z17240_at  L49218_f_at  M71243_f_at  \\\n",
       "count  601.000000  601.000000  601.000000   601.000000   601.000000   \n",
       "mean     0.590949    0.503939    0.370512     0.498539     0.537380   \n",
       "std      0.164910    0.138239    0.138930     0.161129     0.154759   \n",
       "min      0.000000    0.000000    0.000000     0.000000     0.000000   \n",
       "25%      0.506111    0.410831    0.280240     0.405717     0.445447   \n",
       "50%      0.614133    0.492102    0.358113     0.496963     0.521019   \n",
       "75%      0.698660    0.583209    0.442772     0.600409     0.633784   \n",
       "max      1.000000    1.000000    1.000000     1.000000     1.000000   \n",
       "\n",
       "       Z78285_f_at  \n",
       "count   601.000000  \n",
       "mean      0.461455  \n",
       "std       0.153690  \n",
       "min       0.000000  \n",
       "25%       0.356357  \n",
       "50%       0.459238  \n",
       "75%       0.556818  \n",
       "max       1.000000  \n",
       "\n",
       "[8 rows x 7129 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler().fit(X_train)\n",
    "X_train, X_test = [pd.DataFrame(scaler.transform(X), columns = X.columns) for X in (X_train, X_test)]\n",
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2 Fit an 'unregularized' multiple logistic regression model (set C=100000) with all the gene predictors from the data set (you did this in HW4).  Report the misclassification rate on both the train and test sets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_unreg = LogisticRegression(C=100000, solver='lbfgs', max_iter=1000).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassification on train: 0.00\n",
      "Misclassification on test: 0.23\n"
     ]
    }
   ],
   "source": [
    "print('Misclassification on train: %.2f' % (1-accuracy_score(log_unreg.predict(X_train), y_train)))\n",
    "print('Misclassification on test: %.2f' % (1-accuracy_score(log_unreg.predict(X_test), y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3 Use regularization to improve the predictions from the multiple logistic regression model. Specifically, use LASSO-like regularization and cross-validation to train the model on the training set (you did this in HW4).  eport the misclassification rate on both the train and test sets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegressionCV(cv=5, solver='liblinear', max_iter=1000, penalty='l1').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassification on train: 0.09\n",
      "Misclassification on test: 0.14\n"
     ]
    }
   ],
   "source": [
    "print('Misclassification on train: %.2f' % (1-accuracy_score(log_reg.predict(X_train), y_train)))\n",
    "print('Misclassification on test: %.2f' % (1-accuracy_score(log_reg.predict(X_test), y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.4 Use 5-fold cross-validation to select $k$ for a $k$-NN classification model based on the full predictor set.  Choose between `ks = [1,2,3,4,5,7,9,10,15,25]`.  Report your chosen $k$, and report the misclassification rate on both the train and test sets for the model using your chosen $k$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = [1,2,3,4,5,7,9,10,15,25]\n",
    "classifiers = [KNeighborsClassifier(n_neighbors=ki).fit(X_train, y_train) for ki in ks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [cross_val_score(cl, X_train, y_train, cv=5) for cl in classifiers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassification on train: 0.17\n",
      "Misclassification on test: 0.21\n"
     ]
    }
   ],
   "source": [
    "best_i = np.argmin([np.mean(arr) for arr in scores])\n",
    "best_knn = classifiers[best_i]\n",
    "print('Misclassification on train: %.2f' % (1-accuracy_score(best_knn.predict(X_train), y_train)))\n",
    "print('Misclassification on test: %.2f' % (1-accuracy_score(best_knn.predict(X_test), y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.5 Provide the confusion matrix for all 3 models above and report the false positive and false negative rates (all in the test set).  Briefly interpret what you notice.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'><b> Question 2 [20 pts]: Performing Principal Components Analysis </b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1** Create the full PCA decomposition of `X_train` and apply the transformation to both `X_train` and `X_test`.  Report the shape of both of these.  What is the limiting factor for the maximum number of PCA components for this data set? \n",
    "\n",
    "**2.2** PCA is often solely used to help in visualizing high-dimensional problems.  Plot the scatterplot of the second PCA vector of train on the $Y$-axis and the first PCA vector of train on the $X$-axis (be sure to denote the classes via different color/markings).  In 2-3 sentences, explain why using the scatterplot of the top 2 PCA vectors is a useful approach to visualize a high dimensional classification problem.\n",
    "\n",
    "**2.3** Determine and report the variance explained in `X_train` based on the top 2 PCA vectors.  Determine and report how many PCA vectors are needed so that 90\\% of the variability in the predictors is explained, and create a plot to illustrate this result (Hint: look at cumulative explained variability vs. number of PCA components used).\n",
    "\n",
    "**2.4** Plot explained variability in the predictors on the $Y$-axis and the PCA component number on the $X$-axis. Select a reasonable value for the number of components that balances representativeness (of the predictors) with parsimony and justify your choice with 1-3 sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1 Create the full PCA decomposition of X_train and apply the transformation to both X_train and X_test. Report the shape of both of these. What is the limiting factor for the maximum number of PCA components for this data set?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2 PCA is often solely used to help in visualizing high-dimensional problems. Plot the scatterplot of the second PCA vector on the  ùëå -axis and the first PCA vector on the  ùëã -axis (be sure to denote the classes via different color/markings). In 2-3 sentences, explain why using the scatterplot of the top 2 PCA vectors is a useful approach to visualize a high dimensional classification problem.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3 Determine and report the variance explained in `X_train` based on the top 2 PCA vectors.  Determine and report how many PCA vectors are needed so that 90\\% of the variability in the predictors is explained, and create a plot to illustrate this result (Hint: look at cumulative explained variability vs. number of PCA components used).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4 Plot explained variability in the predictors on the  ùëå -axis and the PCA component number on the ùëã -axis. Select a reasonable value for the number of components that balances representativeness (of the predictors) with parsimony and ustify your choice in 1-3 sentences.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'><b> Question 3 [30 pts]: Principal Components Regression (PCR) </b></div>\n",
    "\n",
    "**3.1** Fit three separate Logistic Regression models using principal components as the predictors: (1) with just the first 2 PCA vectors, (2) with the number of component vectors your chose from 2.4 above, and (3) with the number of components that explain at least 90% of the variability in the predictor set. How do the classification accuracy values on both the training and test sets compare with the models fit in Question 1?\n",
    "\n",
    "**3.2** Use cross-validation to determine the best number of principal components. Try out the 3 values from the previous sub-part and optionally include other values as well. For the best performing model according to cross-validation, interpret what the model says about the relationship between `M31523_at` and `Cancer_type`.\n",
    "\n",
    "**3.3** Another way to evaluate models in a classification setting is through an Area-under-the-ROC-Curve (AUC). Briefly explain what the AUC and the ROC are trying to do and how this approach differs (and may be prefered) from evaluating models based on misclassification rate (as you have done thus far in this problem set).\n",
    "\n",
    "**3.4** Evaluate all 6 predictive models (7 if you optionally found a better number of principal components in 3.2) seen in problems 1 and 3 via Area-under-the-ROC-Curve (AUC) on the test set. For the model with the best AUC, plot the ROC. Decide an appropriate strategy for breaking ties, if there are any. Briefly interpret your plot.\n",
    "\n",
    "**3.5** Based on AUC, is there one clear stand-out winner or are a lot of models similar in prediction?  If you were to predict real cancer patients, how would use these models to predict cancer type?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1 Fit three separate Logistic Regression models using principal components as the predictors: (1) with just the first 2 PCA vectors, (2) with the number of component vectors your chose from 2.4 above, and (3) with the number of components that explain at least 90% of the variability in the predictor set.  How do the classification accuracy values on both the training and test sets compare with the models fit in Question 1?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2 Use cross-validation to determine the best number of principal components. Try out the 3 values from the previous sub-part and optionally include other values as well. For the best performing model according to cross-validation, interpret what the model says about the relationship between `M31523_at` and `Cancer_type`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3 Another way to evaluate models in a classification setting is through an Area-under-the-ROC-Curve (AUC). Briefly explain what the AUC and the ROC are trying to do and how this approach differs (and may be prefered) from evaluating models based on misclassification rate (as you have done thus far in this problem set).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.4 Evaluate all 6 predictive models (7 if you optionally found a better number of principal components in 3.2) seen in problems 1 and 3 via Area-under-the-ROC-Curve (AUC) on the test set. For the best model with the best AUC, plot the ROC. Decide an appropriate strategy for breaking ties, if there are any. Briefly interpret your plot.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.5 Based on AUC, is there one clear stand-out winner or are a lot of models similar in prediction? If you were to predict real cancer patients, how would use these models to predict cancer type?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'><b> Question 4: COMPAS [30 points] </b></div>\n",
    "\n",
    "This problem is unrelated to the first 3 problems, and steps through analyzing the COMPAS data set `compas.csv`.  The variables are roughly explained in the `compas_datadict.csv` file, and ProPublica's analysis is publically available here: https://github.com/propublica/compas-analysis.\n",
    "\n",
    "**4.1** Split the data into 80% training and 20% test stratified by race. Before splitting you may want to process the categorical predictors first so you don't have to do it separately in tain and test later.\n",
    "\n",
    "**4.2** Do a little EDA: explore what variables are related to race. Report 3 variables that appear to have significant differences between Caucasians and African Americans, and provide visuals and numerical summaries to support these chosen variables.\n",
    "\n",
    "**4.3** With respect to these 3 chosen variables, how could bias in the data or data collection be impacting or causing these differences?  Explain in 3-5 sentences.\n",
    "\n",
    "**4.4**. Build 2 seperate models to predict recidivism (`two_year_recid`) from these data (2 different 'classes' of models: $k$-NN, linear, logistic, etc.) and be sure to include race as a predictor. \n",
    "\n",
    "**4.5**. Refit the models in 4.4 without race as a predictor.  Report the overall misclassifcation rates, and compare the differences in false positive and false negative rates between Caucasians and African Americans in these models on the test set.  How do these compare to the models when race is included as a predictor?\n",
    "\n",
    "**4.6**. Given your exploration and modeling of the data, should a predictive tool be trusted to be unbiased even if it doesn‚Äôt explicitly use a variable such as race to predict future crime?  Why or why not?  Why is the bias still occurring or not occurring?  Explain in 3-6 sentences total."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.1 Split the data into 80% training and 20% test stratified by race.  Before splitting you may want to process the categorical predictors first so you don't have to do it separately in tain and test later.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compas_df = pd.read_csv('data/compas.csv')\n",
    "\n",
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.2 Do a little EDA: explore what variables are related to race. Report 3 variables that appear to have significant differences between Caucasians and African Americans, and provide visuals and numerical summaries to support these chosen variables.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.3 With respect to these 3 chosen variables, how could bias in the data or data collection be impacting or causing these differences? Explain in 3-5 sentences.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.4. Build 2 seperate models to predict recidivism (`two_year_recid`) from these data (2 different 'classes' of models:  $k$-NN, linear, logistic, etc.) and be sure to include race as a predictor.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.5. Refit the models in 4.4 without race as a predictor. Report the overall misclassifcation rates, and compare the differences in false positive and false negative rates between Caucasians and African Americans in these models on the test set. How do these compare to the models when race is included as a predictor?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.6. Given your exploration and modeling of the data, should a predictive tool be trusted to be unbiased even if it doesn‚Äôt explicitly use a variable such as race to predict future crime?  Why or why not?  Why is the bias still occurring or not occurring?  Explain in 3-6 sentences total.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
