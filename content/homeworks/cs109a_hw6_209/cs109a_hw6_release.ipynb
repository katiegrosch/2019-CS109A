{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS109A Introduction to Data Science: \n",
    "\n",
    "## Homework 6: Trees, Bagging, Random Forests, and Boosting\n",
    "\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2019**<br/>\n",
    "**Instructors**: Pavlos Protopapas, Kevin Rader and Chris Tanner\n",
    "\n",
    "<hr style=\"height:2pt\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "blockquote { background: #AEDE94; }\n",
       "h1 { \n",
       "    padding-top: 25px;\n",
       "    padding-bottom: 25px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "h2 { \n",
       "    padding-top: 10px;\n",
       "    padding-bottom: 10px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "\n",
       "div.exercise {\n",
       "\tbackground-color: #ffcccc;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "span.sub-q {\n",
       "\tfont-weight: bold;\n",
       "}\n",
       "div.theme {\n",
       "\tbackground-color: #DDDDDD;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 18pt;\n",
       "}\n",
       "div.gc { \n",
       "\tbackground-color: #AEDE94;\n",
       "\tborder-color: #E9967A; \t \n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 12pt;\n",
       "}\n",
       "p.q1 { \n",
       "    padding-top: 5px;\n",
       "    padding-bottom: 5px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "header {\n",
       "   padding-top: 35px;\n",
       "    padding-bottom: 35px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RUN THIS CELL \n",
    "import requests\n",
    "from IPython.core.display import HTML, display\n",
    "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INSTRUCTIONS\n",
    "\n",
    "- To submit your assignment follow the instructions given in Canvas.\n",
    "- Restart the kernel and run the whole notebook again before you submit. \n",
    "- As much as possible, try and stick to the hints and functions we import at the top of the homework, as those are the ideas and tools the class supports and is aiming to teach. And if a problem specifies a particular library you're required to use that library, and possibly others from the import list.\n",
    "- Please use .head() when viewing data. Do not submit a notebook that is excessively long because output was not suppressed or otherwise limited. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</span></div><div class='alert alert-block alert-danger'><b>WARNING!:</b><span style = 'color:black'> Do not delete any of the `### cs109Test() ###` comment lines!</span></div></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</span></div><div class='alert alert-block alert-danger'><b>Note:</b><span style = 'color:black'> Make sure your submission passes all assert statements we've provided in this notebook.</span></div></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.utils import resample\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(style='whitegrid')\n",
    "pd.set_option('display.width', 1500)\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Higgs Boson Discovery\n",
    "\n",
    "The discovery of the Higgs boson in July 2012 marked a fundamental breakthrough in particle physics. The Higgs boson particle was discovered through experiments at the Large Hadron Collider at CERN, by colliding beams of protons at high energy. A key challenge in analyzing the results of these experiments is to differentiate between collisions that produce Higgs bosons and collisions that produce only background noise. \n",
    "\n",
    "You are provided with data from Monte-Carlo simulations of collisions of particles in a particle collider experiment. The training set is available in `Higgs_train.csv` and the test set is in `Higgs_test.csv`. Each row in these files corresponds to a particle collision described by 28 features (columns 1-28), of which the first 21 features are kinematic properties measured by the particle detectors in the accelerator, and the remaining features are derived by physicists from the first 21 features. The class label is provided in the last column, with a label of 1 indicating that the collision produces Higgs bosons (signal), and a label of 0 indicating that the collision produces other particles (background).\n",
    "\n",
    "The data set provided to you is a small subset of the HIGGS data set in the UCI machine learning repository. The following paper contains further details about the data set and the predictors used: [Baldi et al., Nature Communications 5, 2014](https://www.nature.com/articles/ncomms5308)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 training samples, 5000 test samples\n",
      "\n",
      "Columns:\n",
      "lepton pT, lepton eta, lepton phi, missing energy magnitude, missing energy phi, jet 1 pt, jet 1 eta, jet 1 phi, jet 1 b-tag, jet 2 pt, jet 2 eta, jet 2 phi, jet 2 b-tag, jet 3 pt, jet 3 eta, jet 3 phi, jet 3 b-tag, jet 4 pt, jet 4 eta, jet 4 phi, jet 4 b-tag, m_jj, m_jjj, m_lv, m_jlv, m_bb, m_wbb, m_wwbb, class\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "data_train = pd.read_csv('data/Higgs_train.csv')\n",
    "data_test = pd.read_csv('data/Higgs_test.csv')\n",
    "\n",
    "print(f\"{len(data_train)} training samples, {len(data_test)} test samples\")\n",
    "print(\"\\nColumns:\")\n",
    "print(', '.join(data_train.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lepton pT</th>\n",
       "      <th>lepton eta</th>\n",
       "      <th>lepton phi</th>\n",
       "      <th>missing energy magnitude</th>\n",
       "      <th>missing energy phi</th>\n",
       "      <th>jet 1 pt</th>\n",
       "      <th>jet 1 eta</th>\n",
       "      <th>jet 1 phi</th>\n",
       "      <th>jet 1 b-tag</th>\n",
       "      <th>jet 2 pt</th>\n",
       "      <th>jet 2 eta</th>\n",
       "      <th>jet 2 phi</th>\n",
       "      <th>jet 2 b-tag</th>\n",
       "      <th>jet 3 pt</th>\n",
       "      <th>jet 3 eta</th>\n",
       "      <th>jet 3 phi</th>\n",
       "      <th>jet 3 b-tag</th>\n",
       "      <th>jet 4 pt</th>\n",
       "      <th>jet 4 eta</th>\n",
       "      <th>jet 4 phi</th>\n",
       "      <th>jet 4 b-tag</th>\n",
       "      <th>m_jj</th>\n",
       "      <th>m_jjj</th>\n",
       "      <th>m_lv</th>\n",
       "      <th>m_jlv</th>\n",
       "      <th>m_bb</th>\n",
       "      <th>m_wbb</th>\n",
       "      <th>m_wwbb</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.377</td>\n",
       "      <td>-1.5800</td>\n",
       "      <td>-1.7100</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.114</td>\n",
       "      <td>1.250</td>\n",
       "      <td>0.620</td>\n",
       "      <td>-1.480</td>\n",
       "      <td>2.17</td>\n",
       "      <td>0.754</td>\n",
       "      <td>0.7750</td>\n",
       "      <td>-0.667</td>\n",
       "      <td>2.21</td>\n",
       "      <td>1.280</td>\n",
       "      <td>-1.190</td>\n",
       "      <td>0.505</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.110</td>\n",
       "      <td>-0.464</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.522</td>\n",
       "      <td>1.320</td>\n",
       "      <td>0.982</td>\n",
       "      <td>1.360</td>\n",
       "      <td>0.965</td>\n",
       "      <td>1.310</td>\n",
       "      <td>1.080</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.707</td>\n",
       "      <td>0.0876</td>\n",
       "      <td>-0.4000</td>\n",
       "      <td>0.919</td>\n",
       "      <td>-1.230</td>\n",
       "      <td>1.170</td>\n",
       "      <td>-0.553</td>\n",
       "      <td>0.886</td>\n",
       "      <td>2.17</td>\n",
       "      <td>1.300</td>\n",
       "      <td>0.7620</td>\n",
       "      <td>-1.060</td>\n",
       "      <td>2.21</td>\n",
       "      <td>0.607</td>\n",
       "      <td>0.459</td>\n",
       "      <td>1.020</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.497</td>\n",
       "      <td>0.956</td>\n",
       "      <td>0.236</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.829</td>\n",
       "      <td>0.992</td>\n",
       "      <td>1.160</td>\n",
       "      <td>2.220</td>\n",
       "      <td>1.190</td>\n",
       "      <td>0.938</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.617</td>\n",
       "      <td>0.2660</td>\n",
       "      <td>-1.3500</td>\n",
       "      <td>1.150</td>\n",
       "      <td>1.040</td>\n",
       "      <td>0.955</td>\n",
       "      <td>0.377</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.060</td>\n",
       "      <td>-0.0194</td>\n",
       "      <td>1.110</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.470</td>\n",
       "      <td>0.205</td>\n",
       "      <td>-1.060</td>\n",
       "      <td>2.55</td>\n",
       "      <td>1.490</td>\n",
       "      <td>-0.398</td>\n",
       "      <td>-0.542</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.020</td>\n",
       "      <td>1.030</td>\n",
       "      <td>0.986</td>\n",
       "      <td>0.928</td>\n",
       "      <td>1.370</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.917</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.851</td>\n",
       "      <td>-0.3810</td>\n",
       "      <td>-0.0713</td>\n",
       "      <td>1.470</td>\n",
       "      <td>-0.795</td>\n",
       "      <td>0.692</td>\n",
       "      <td>0.883</td>\n",
       "      <td>0.497</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.620</td>\n",
       "      <td>0.1240</td>\n",
       "      <td>1.180</td>\n",
       "      <td>1.11</td>\n",
       "      <td>1.290</td>\n",
       "      <td>0.160</td>\n",
       "      <td>-0.916</td>\n",
       "      <td>2.55</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.796</td>\n",
       "      <td>-1.520</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.200</td>\n",
       "      <td>1.100</td>\n",
       "      <td>0.987</td>\n",
       "      <td>1.350</td>\n",
       "      <td>1.460</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.954</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.768</td>\n",
       "      <td>-0.6920</td>\n",
       "      <td>-0.0402</td>\n",
       "      <td>0.615</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.749</td>\n",
       "      <td>0.397</td>\n",
       "      <td>-0.874</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.150</td>\n",
       "      <td>0.1270</td>\n",
       "      <td>1.320</td>\n",
       "      <td>2.21</td>\n",
       "      <td>0.730</td>\n",
       "      <td>-0.758</td>\n",
       "      <td>-1.120</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.848</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.502</td>\n",
       "      <td>1.55</td>\n",
       "      <td>0.922</td>\n",
       "      <td>0.864</td>\n",
       "      <td>0.983</td>\n",
       "      <td>1.370</td>\n",
       "      <td>0.601</td>\n",
       "      <td>0.919</td>\n",
       "      <td>0.957</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   lepton pT  lepton eta  lepton phi  missing energy magnitude  missing energy phi  jet 1 pt  jet 1 eta  jet 1 phi  jet 1 b-tag  jet 2 pt  jet 2 eta  jet 2 phi  jet 2 b-tag  jet 3 pt  jet 3 eta  jet 3 phi  jet 3 b-tag  jet 4 pt  jet 4 eta  jet 4 phi  jet 4 b-tag   m_jj  m_jjj   m_lv  m_jlv   m_bb  m_wbb  m_wwbb  class\n",
       "0      0.377     -1.5800     -1.7100                     0.991               0.114     1.250      0.620     -1.480         2.17     0.754     0.7750     -0.667         2.21     1.280     -1.190      0.505         0.00     1.110     -0.464      0.397         0.00  0.522  1.320  0.982  1.360  0.965  1.310   1.080    1.0\n",
       "1      0.707      0.0876     -0.4000                     0.919              -1.230     1.170     -0.553      0.886         2.17     1.300     0.7620     -1.060         2.21     0.607      0.459      1.020         0.00     0.497      0.956      0.236         0.00  0.440  0.829  0.992  1.160  2.220  1.190   0.938    1.0\n",
       "2      0.617      0.2660     -1.3500                     1.150               1.040     0.955      0.377     -0.148         0.00     1.060    -0.0194      1.110         0.00     1.470      0.205     -1.060         2.55     1.490     -0.398     -0.542         0.00  1.020  1.030  0.986  0.928  1.370  0.982   0.917    1.0\n",
       "3      0.851     -0.3810     -0.0713                     1.470              -0.795     0.692      0.883      0.497         0.00     1.620     0.1240      1.180         1.11     1.290      0.160     -0.916         2.55     0.945      0.796     -1.520         0.00  1.200  1.100  0.987  1.350  1.460  0.995   0.954    1.0\n",
       "4      0.768     -0.6920     -0.0402                     0.615               0.144     0.749      0.397     -0.874         0.00     1.150     0.1270      1.320         2.21     0.730     -0.758     -1.120         0.00     0.848      0.107      0.502         1.55  0.922  0.864  0.983  1.370  0.601  0.919   0.957    0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lepton pT</th>\n",
       "      <th>lepton eta</th>\n",
       "      <th>lepton phi</th>\n",
       "      <th>missing energy magnitude</th>\n",
       "      <th>missing energy phi</th>\n",
       "      <th>jet 1 pt</th>\n",
       "      <th>jet 1 eta</th>\n",
       "      <th>jet 1 phi</th>\n",
       "      <th>jet 1 b-tag</th>\n",
       "      <th>jet 2 pt</th>\n",
       "      <th>jet 2 eta</th>\n",
       "      <th>jet 2 phi</th>\n",
       "      <th>jet 2 b-tag</th>\n",
       "      <th>jet 3 pt</th>\n",
       "      <th>jet 3 eta</th>\n",
       "      <th>jet 3 phi</th>\n",
       "      <th>jet 3 b-tag</th>\n",
       "      <th>jet 4 pt</th>\n",
       "      <th>jet 4 eta</th>\n",
       "      <th>jet 4 phi</th>\n",
       "      <th>jet 4 b-tag</th>\n",
       "      <th>m_jj</th>\n",
       "      <th>m_jjj</th>\n",
       "      <th>m_lv</th>\n",
       "      <th>m_jlv</th>\n",
       "      <th>m_bb</th>\n",
       "      <th>m_wbb</th>\n",
       "      <th>m_wwbb</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.978645</td>\n",
       "      <td>-0.014280</td>\n",
       "      <td>-0.018956</td>\n",
       "      <td>1.005793</td>\n",
       "      <td>0.002528</td>\n",
       "      <td>0.980390</td>\n",
       "      <td>0.025014</td>\n",
       "      <td>-0.007104</td>\n",
       "      <td>0.993678</td>\n",
       "      <td>0.988659</td>\n",
       "      <td>-0.010310</td>\n",
       "      <td>-0.006926</td>\n",
       "      <td>1.006922</td>\n",
       "      <td>0.997004</td>\n",
       "      <td>0.018817</td>\n",
       "      <td>0.003952</td>\n",
       "      <td>1.011994</td>\n",
       "      <td>0.982806</td>\n",
       "      <td>0.005201</td>\n",
       "      <td>0.003349</td>\n",
       "      <td>1.007810</td>\n",
       "      <td>1.038431</td>\n",
       "      <td>1.027201</td>\n",
       "      <td>1.054719</td>\n",
       "      <td>1.023094</td>\n",
       "      <td>0.958464</td>\n",
       "      <td>1.033432</td>\n",
       "      <td>0.960494</td>\n",
       "      <td>0.524600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.547025</td>\n",
       "      <td>1.011927</td>\n",
       "      <td>0.997945</td>\n",
       "      <td>0.591907</td>\n",
       "      <td>1.003337</td>\n",
       "      <td>0.463677</td>\n",
       "      <td>1.002018</td>\n",
       "      <td>1.014559</td>\n",
       "      <td>1.028920</td>\n",
       "      <td>0.476462</td>\n",
       "      <td>1.007983</td>\n",
       "      <td>1.002177</td>\n",
       "      <td>1.045206</td>\n",
       "      <td>0.471681</td>\n",
       "      <td>1.007824</td>\n",
       "      <td>0.999656</td>\n",
       "      <td>1.200416</td>\n",
       "      <td>0.497681</td>\n",
       "      <td>1.007999</td>\n",
       "      <td>1.008904</td>\n",
       "      <td>1.400846</td>\n",
       "      <td>0.619460</td>\n",
       "      <td>0.353984</td>\n",
       "      <td>0.173243</td>\n",
       "      <td>0.427141</td>\n",
       "      <td>0.495720</td>\n",
       "      <td>0.352966</td>\n",
       "      <td>0.306057</td>\n",
       "      <td>0.499444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.275000</td>\n",
       "      <td>-2.410000</td>\n",
       "      <td>-1.740000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>-1.740000</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>-2.920000</td>\n",
       "      <td>-1.740000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.198000</td>\n",
       "      <td>-2.910000</td>\n",
       "      <td>-1.740000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.265000</td>\n",
       "      <td>-2.720000</td>\n",
       "      <td>-1.740000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.366000</td>\n",
       "      <td>-2.500000</td>\n",
       "      <td>-1.740000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.151000</td>\n",
       "      <td>0.443000</td>\n",
       "      <td>0.339000</td>\n",
       "      <td>0.371000</td>\n",
       "      <td>0.079500</td>\n",
       "      <td>0.413000</td>\n",
       "      <td>0.452000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.587000</td>\n",
       "      <td>-0.764250</td>\n",
       "      <td>-0.877500</td>\n",
       "      <td>0.581000</td>\n",
       "      <td>-0.870000</td>\n",
       "      <td>0.676000</td>\n",
       "      <td>-0.659250</td>\n",
       "      <td>-0.885000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666000</td>\n",
       "      <td>-0.699000</td>\n",
       "      <td>-0.859500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.664750</td>\n",
       "      <td>-0.679250</td>\n",
       "      <td>-0.858000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.619000</td>\n",
       "      <td>-0.707250</td>\n",
       "      <td>-0.869250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.798750</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.986000</td>\n",
       "      <td>0.768000</td>\n",
       "      <td>0.672000</td>\n",
       "      <td>0.826000</td>\n",
       "      <td>0.772750</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.846000</td>\n",
       "      <td>-0.009305</td>\n",
       "      <td>-0.016050</td>\n",
       "      <td>0.903500</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.891000</td>\n",
       "      <td>0.049500</td>\n",
       "      <td>-0.023500</td>\n",
       "      <td>1.090000</td>\n",
       "      <td>0.891000</td>\n",
       "      <td>-0.004800</td>\n",
       "      <td>-0.030700</td>\n",
       "      <td>1.110000</td>\n",
       "      <td>0.899500</td>\n",
       "      <td>0.045700</td>\n",
       "      <td>0.018800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.877000</td>\n",
       "      <td>0.012900</td>\n",
       "      <td>-0.004700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.898000</td>\n",
       "      <td>0.957000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.922000</td>\n",
       "      <td>0.868000</td>\n",
       "      <td>0.952000</td>\n",
       "      <td>0.877500</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.220000</td>\n",
       "      <td>0.725500</td>\n",
       "      <td>0.837000</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>0.866000</td>\n",
       "      <td>1.160000</td>\n",
       "      <td>0.716000</td>\n",
       "      <td>0.894000</td>\n",
       "      <td>2.170000</td>\n",
       "      <td>1.192500</td>\n",
       "      <td>0.692250</td>\n",
       "      <td>0.855500</td>\n",
       "      <td>2.210000</td>\n",
       "      <td>1.232500</td>\n",
       "      <td>0.717000</td>\n",
       "      <td>0.855000</td>\n",
       "      <td>2.550000</td>\n",
       "      <td>1.220000</td>\n",
       "      <td>0.719000</td>\n",
       "      <td>0.859000</td>\n",
       "      <td>3.100000</td>\n",
       "      <td>1.030000</td>\n",
       "      <td>1.090000</td>\n",
       "      <td>1.030000</td>\n",
       "      <td>1.160000</td>\n",
       "      <td>1.120000</td>\n",
       "      <td>1.140000</td>\n",
       "      <td>1.060000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.330000</td>\n",
       "      <td>2.430000</td>\n",
       "      <td>1.740000</td>\n",
       "      <td>6.260000</td>\n",
       "      <td>1.740000</td>\n",
       "      <td>4.190000</td>\n",
       "      <td>2.960000</td>\n",
       "      <td>1.740000</td>\n",
       "      <td>2.170000</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>2.910000</td>\n",
       "      <td>1.740000</td>\n",
       "      <td>2.210000</td>\n",
       "      <td>4.630000</td>\n",
       "      <td>2.730000</td>\n",
       "      <td>1.740000</td>\n",
       "      <td>2.550000</td>\n",
       "      <td>5.770000</td>\n",
       "      <td>2.490000</td>\n",
       "      <td>1.740000</td>\n",
       "      <td>3.100000</td>\n",
       "      <td>10.600000</td>\n",
       "      <td>5.740000</td>\n",
       "      <td>3.940000</td>\n",
       "      <td>6.220000</td>\n",
       "      <td>5.080000</td>\n",
       "      <td>4.320000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         lepton pT   lepton eta   lepton phi  missing energy magnitude  missing energy phi     jet 1 pt    jet 1 eta    jet 1 phi  jet 1 b-tag     jet 2 pt    jet 2 eta    jet 2 phi  jet 2 b-tag     jet 3 pt    jet 3 eta    jet 3 phi  jet 3 b-tag     jet 4 pt    jet 4 eta    jet 4 phi  jet 4 b-tag         m_jj        m_jjj         m_lv        m_jlv         m_bb        m_wbb       m_wwbb        class\n",
       "count  5000.000000  5000.000000  5000.000000               5000.000000         5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000\n",
       "mean      0.978645    -0.014280    -0.018956                  1.005793            0.002528     0.980390     0.025014    -0.007104     0.993678     0.988659    -0.010310    -0.006926     1.006922     0.997004     0.018817     0.003952     1.011994     0.982806     0.005201     0.003349     1.007810     1.038431     1.027201     1.054719     1.023094     0.958464     1.033432     0.960494     0.524600\n",
       "std       0.547025     1.011927     0.997945                  0.591907            1.003337     0.463677     1.002018     1.014559     1.028920     0.476462     1.007983     1.002177     1.045206     0.471681     1.007824     0.999656     1.200416     0.497681     1.007999     1.008904     1.400846     0.619460     0.353984     0.173243     0.427141     0.495720     0.352966     0.306057     0.499444\n",
       "min       0.275000    -2.410000    -1.740000                  0.010000           -1.740000     0.170000    -2.920000    -1.740000     0.000000     0.198000    -2.910000    -1.740000     0.000000     0.265000    -2.720000    -1.740000     0.000000     0.366000    -2.500000    -1.740000     0.000000     0.151000     0.443000     0.339000     0.371000     0.079500     0.413000     0.452000     0.000000\n",
       "25%       0.587000    -0.764250    -0.877500                  0.581000           -0.870000     0.676000    -0.659250    -0.885000     0.000000     0.666000    -0.699000    -0.859500     0.000000     0.664750    -0.679250    -0.858000     0.000000     0.619000    -0.707250    -0.869250     0.000000     0.798750     0.850000     0.986000     0.768000     0.672000     0.826000     0.772750     0.000000\n",
       "50%       0.846000    -0.009305    -0.016050                  0.903500            0.001300     0.891000     0.049500    -0.023500     1.090000     0.891000    -0.004800    -0.030700     1.110000     0.899500     0.045700     0.018800     0.000000     0.877000     0.012900    -0.004700     0.000000     0.898000     0.957000     0.990000     0.922000     0.868000     0.952000     0.877500     1.000000\n",
       "75%       1.220000     0.725500     0.837000                  1.300000            0.866000     1.160000     0.716000     0.894000     2.170000     1.192500     0.692250     0.855500     2.210000     1.232500     0.717000     0.855000     2.550000     1.220000     0.719000     0.859000     3.100000     1.030000     1.090000     1.030000     1.160000     1.120000     1.140000     1.060000     1.000000\n",
       "max       5.330000     2.430000     1.740000                  6.260000            1.740000     4.190000     2.960000     1.740000     2.170000     4.800000     2.910000     1.740000     2.210000     4.630000     2.730000     1.740000     2.550000     5.770000     2.490000     1.740000     3.100000    10.600000     5.740000     3.940000     6.220000     5.080000     4.320000     3.500000     1.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(data_train.head())\n",
    "display(data_train.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split into NumPy arrays\n",
    "X_train = data_train.iloc[:, data_train.columns != 'class'].values\n",
    "y_train = data_train['class'].values\n",
    "X_test = data_test.iloc[:, data_test.columns != 'class'].values\n",
    "y_test = data_test['class'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'><b>Question 1 [20pts]: A Single Model </b></div>\n",
    "We start by fitting a basic model we can compare the other models to. We will pick a decision tree as the base model because we will later include bagging, random forests and boosting and want a fair comparison. We will tune the decision tree using cross-validation (of course). We will be tuning the maximum tree depth; we refer to this parameter as \"depth\" for simplicity.\n",
    "\n",
    "Since we will only be using tree-based methods in this homework, we do not need to standardize or normalize the predictors.\n",
    "\n",
    "\n",
    "**1.1** Fit a decision tree model to the training set. Choose a range of tree depths from 1 to 20 and evaluate the  performance and standard deviations for each depth using 5-fold cross-validation. Plot the estimated mean +/- 2 standard deviations for each depth. Also, include the training set performance in your plot, but set the y-axis to focus on the cross-validation performance. \n",
    "Store the CV means and std variables `cvmeans`, `cvstds` and the train score `train_scores`  \n",
    "\n",
    "*Hint*: use `plt.fill_between` to shade the region.\n",
    "\n",
    "**1.2** Select an appropriate depth and justify your choice using your cross-validation estimates. Then report the classification accuracy on the **test set**. Store the training and test accuracies in variables named `best_cv_tree_train_score` and `best_cv_tree_test_score` to refer to in a later question.\n",
    "\n",
    "**1.3** What is the mechanism by which limiting the depth of the tree avoids over-fitting? What is one downside of limiting the tree depth? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1** Fit a decision tree model to the training set. Choose a range of tree depths from 1 to 20 and evaluate the  performance and standard deviations for each depth using 5-fold cross-validation. Plot the estimated mean +/- 2 standard deviations for each depth. Also, include the training set performance in your plot, but set the y-axis to focus on the cross-validation performance.\n",
    "Store the CV means and std variables `cvmeans`, `cvstds` and the train score `train_scores`\n",
    "\n",
    "*Hint*: use `plt.fill_between` to shade the region.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</span></div><div class='alert alert-block alert-danger'><b>WARNING!:</b><span style = 'color:black'> Do not delete any of the `### cs109Test() ###` comment lines!</span></div></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### cs109Test(test_1.1a) ###\n",
    "\n",
    "depths = list(range(1, 21))\n",
    "\n",
    "def calc_meanstd(X_train, y_train, depths):\n",
    "    cvmeans, cvstds, train_scores = [], [], []\n",
    "    for d in depths:\n",
    "        tree = DecisionTreeClassifier(max_depth=d)\n",
    "        cvs = cross_val_score(tree, X_train, y_train, cv=5)\n",
    "        cvmeans.append(np.mean(cvs))\n",
    "        cvstds.append(np.std(cvs))\n",
    "        train_scores.append(tree.fit(X_train, y_train).score(X_train, y_train))\n",
    "    return cvmeans, cvstds, train_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</span></div><div class='alert alert-block alert-danger'><b>WARNING!:</b><span style = 'color:black'> Do not delete any of the `### cs109Test() ###` comment lines!</span></div></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### cs109Test(test_1.1b) ###\n",
    "# name the CV means and std variables cvmeans, cvstds and the train score train_scores  \n",
    "cvmeans, cvstds, train_scores = calc_meanstd(X_train, y_train, depths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAETCAYAAADZHBoWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a2533d400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cvm, cvs = np.array(cvmeans), np.array(cvstds)\n",
    "plt.fill_between(depths, cvm - 2 * cvs, cvm + 2 * cvs, alpha=0.2)\n",
    "plt.plot(depths, cvmeans)\n",
    "plt.xlabel(\"Depth\")\n",
    "plt.ylabel(\"Mean +- 2 standard deviations\")\n",
    "plt.title(\"Cross-validation scores vs. depth of decision tree\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</span></div><div class='alert alert-block alert-danger'><b>Note:</b><span style = 'color:black'> Make sure your submission passes all assert statements we've provided in this notebook.</span></div></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 1.1 Check that you have the requested variables\n",
    "for var in ['train_scores', 'cvmeans', 'cvstds']:\n",
    "    assert var in globals(), f\"Variable '{var}' does not exist!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2** Select an appropriate depth and justify your choice using your cross-validation estimates. Then report the classification accuracy on the **test set**. Store the training and test accuracies in variables named `best_cv_tree_train_score` and `best_cv_tree_test_score` to refer to in a later question.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</span></div><div class='alert alert-block alert-danger'><b>WARNING!:</b><span style = 'color:black'> Do not delete any of the `### cs109Test() ###` comment lines!</span></div></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score on test set:  0.6478\n"
     ]
    }
   ],
   "source": [
    "### cs109Test(test_1.3) ###\n",
    "depth = 5\n",
    "best_tree = DecisionTreeClassifier(max_depth=depth).fit(X_train, y_train)\n",
    "accuracy_score = accuracy_score(y_test, best_tree.predict(X_test))\n",
    "print(\"Accuracy score on test set: \", accuracy_score)\n",
    "best_cv_tree_train_score = best_tree.score(X_train, y_train)\n",
    "best_cv_tree_test_score = best_tree.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since depth 5 obtains the highest cross-validation score, it is reasonable to assume that a depth of 5 strikes a good balance between enabling sufficient complexity and limiting variance. This intuition is reinforced by the fact that train and test accuracy on the resulting classifier are fairly close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 1.2 Check that you have the requested variables\n",
    "for var in ['best_cv_tree_train_score', 'best_cv_tree_test_score']:\n",
    "    assert var in globals(), f\"Variable '{var}' does not exist!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3** What is the mechanism by which limiting the depth of the tree avoids over-fitting? What is one downside of limiting the tree depth?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with PCA, limiting the depth of the tree prevents overfitting by making it impossible to capture all of the variance in the training data, which would always be possible if we did not limit tree depth. As a result, the model captures the most important sources of variation in the data, but avoids some of the noise arising from random error.\n",
    "\n",
    "Of course, limiting the tree depth also restricts the complexity of the model, which may prevent us from capturing real information from some of the less substantive predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'><b> Question 2 [25 pts]: Bagging </b></div> \n",
    "Bagging is the technique of building the same model on multiple bootstrap samples from the data and combining each model's prediction to get an overall classification. In this question we build an example by hand and study how the number of bootstrapped datasets impacts the accuracy of the resulting classification.\n",
    "\n",
    "\n",
    "\n",
    "**2.1** Using decision trees, choose a tree depth that will overfit the training set. What evidence leads you to believe that this depth  overfits? Assign your choice to a variable named `tree_depth` here. (You may want to explore different settings for this value in the problems below.)\n",
    "\n",
    "**2.2** Create 55 bootstrapped replications of the original training data, and fit a decision tree to each. Use the tree depth you just chose in 2.1. Record each tree's prediction. In particular, produce a dataset like those shown (see below), where each row is a training and test observation, respectively, each column is one of the trees, and each entry is that tree's prediction for that observation. \n",
    "\n",
    "Store these results as `bagging_train_df` and `bagging_test_df`. Don't worry about visualizing these results yet.\n",
    "\n",
    "**2.3**  _Aggregate_ all 55 _bootstrapped_ models to get a combined prediction for each training and test point: predict a 1 if and only if a majority of the models predict that example to be from class 1. Assign the bagging accuracy test to a variable name `bagging_accuracy_test`. What accuracy does this *bagging* model achieve on the test set? Write an assertion that verifies that this test-set accuracy is at least as good as the accuracy for the model you fit in Question 1.\n",
    "\n",
    "**2.4** We want to know how the number of bootstraps affects our bagging ensemble's performance. Use the `running_predictions` function (given below) to get the model's accuracy score when using only 1,2,3,4,... of the bootstrapped models. Make a plot of training and test set accuracies as a function of number of bootstraps. \n",
    "** use the depth that you used above ** \n",
    "\n",
    "On your plot, also include horizontal lines for two baselines:\n",
    "- the test accuracy of the best model from question 1\n",
    "- the test accuracy of a single tree with the tree depth you chose in 2.1, trained on the full training set.\n",
    "\n",
    "**2.5** Referring to your graph from 2.4, compare the performance of bagging against the baseline of a single `tree_depth` tree. Explain the differences you see.\n",
    "\n",
    "**2.6** Bagging and limiting tree depth both affect how much the model overfits. Compare and contrast these two approaches. Your answer should refer to your graph in 2.4 and may duplicate something you said in your answer to 1.5.\n",
    "\n",
    "**2.7**: In what ways might our bagging classifier be overfitting the data? In what ways might it be underfitting?\n",
    "\n",
    "**Hints**\n",
    "- Use `resample` from sklearn to easily bootstrap the x and y data.\n",
    "- use `np.mean` to easily test for majority. If a majority of models vote 1, what does that imply about the mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1** Using decision trees, choose a tree depth that will overfit the training set. What evidence leads you to believe that this depth  overfits? Assign your choice to a variable named `tree_depth` here. (You may want to explore different settings for this value in the problems below.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</span></div><div class='alert alert-block alert-danger'><b>WARNING!:</b><span style = 'color:black'> Do not delete any of the `### cs109Test() ###` comment lines!</span></div></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV 0.59, train 1.00, test 0.60\n"
     ]
    }
   ],
   "source": [
    "# cs109Test (test_2.1)\n",
    "# Assign your choice to a variable named tree_depth\n",
    "tree_depth = 20\n",
    "over_tree = DecisionTreeClassifier(max_depth=tree_depth).fit(X_train, y_train)\n",
    "cv_score = np.mean(cross_val_score(over_tree, X_train, y_train, cv=5))\n",
    "train_score = over_tree.score(X_train, y_train)\n",
    "test_score = over_tree.score(X_test, y_test)\n",
    "print(\"CV %.2f, train %.2f, test %.2f\" % (cv_score, train_score, test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see evidence of high variance in the difference between train and test scores. We were able to capture the training data perfectly, but with a disappointing test score of 0.6, this model clearly does not generalize as well as some of the simpler trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 2.1 Check that you have the requested variables\n",
    "assert 'tree_depth' in globals(), f\"Variable 'tree_depth' does not exist!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2** Create 55 bootstrapped replications of the original training data, and fit a decision tree to each. Use the tree depth you just chose in 2.1. Record each tree's prediction. In particular, produce a dataset like those shown (see below), where each row is a training and test observation, respectively, each column is one of the trees, and each entry is that tree's prediction for that observation.\n",
    "\n",
    "Store these results as `bagging_train_df` and `bagging_test_df`. Don't worry about visualizing these results yet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Structure of `bagging_train_df` and `bagging_test_df`:**\n",
    "\n",
    "`bagging_train`:\n",
    "\n",
    "|     |bootstrap model 1's prediction|bootstrap model 2's prediction|...|bootstrap model 45's prediction|  \n",
    "| --- | --- | --- | --- | --- |\n",
    "|training row 1| binary value | binary value|... |binary value|\n",
    "|training row 2| binary value| binary value|... |binary value|\n",
    "|...| ...| ...|... |... |\n",
    "\n",
    "`bagging_test`:\n",
    "\n",
    "|     |bootstrap model 1's prediction|bootstrap model 2's prediction|...|bootstrap model 45's prediction|\n",
    "| --- | --- | --- | --- | --- |\n",
    "|test row 1| binary value | binary value|... |binary value|\n",
    "|test row 2| binary value| binary value|... |binary value|\n",
    "|...| ...| ...|... |... |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</span></div><div class='alert alert-block alert-danger'><b>WARNING!:</b><span style = 'color:black'> Do not delete any of the `### cs109Test() ###` comment lines!</span></div></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### cs109test(test_2.2) ### \n",
    "\n",
    "def bagger(n_trees: int, tree_depth: int,  random_seed=0) -> (pd.DataFrame, pd.DataFrame, list):\n",
    "    \"\"\"A function that takes as \n",
    "    \n",
    "    Inputs:\n",
    "      n_tres\n",
    "      tree_depth \n",
    "      a random_seed (default =0)\n",
    "    \n",
    "    Returns:\n",
    "      bagging_train dataframe (as described above)\n",
    "      bagging_test dataframe (as described above)\n",
    "      bagging_models every trained model for each bootstrap (you will need this in Q3.2)\n",
    "    \"\"\"\n",
    "    bagging_models = []\n",
    "    train_dict, test_dict = dict(), dict()\n",
    "    for t in range(n_trees):\n",
    "        X_train_re, y_train_re = resample(X_train, y_train)\n",
    "        tree = DecisionTreeClassifier(max_depth=tree_depth, random_state=random_seed).fit(X_train_re, y_train_re)\n",
    "        bagging_models.append(tree)\n",
    "        train_dict['Tree %d' % t] = tree.predict(X_train)\n",
    "        test_dict['Tree %d' % t] = tree.predict(X_test)\n",
    "    bagging_train_df, bagging_test_df = pd.DataFrame(train_dict), pd.DataFrame(test_dict)\n",
    "    return bagging_train_df, bagging_test_df, bagging_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tree 0</th>\n",
       "      <th>Tree 1</th>\n",
       "      <th>Tree 10</th>\n",
       "      <th>Tree 11</th>\n",
       "      <th>Tree 12</th>\n",
       "      <th>Tree 13</th>\n",
       "      <th>Tree 14</th>\n",
       "      <th>Tree 15</th>\n",
       "      <th>Tree 16</th>\n",
       "      <th>Tree 17</th>\n",
       "      <th>Tree 18</th>\n",
       "      <th>Tree 19</th>\n",
       "      <th>Tree 2</th>\n",
       "      <th>Tree 20</th>\n",
       "      <th>Tree 21</th>\n",
       "      <th>Tree 22</th>\n",
       "      <th>Tree 23</th>\n",
       "      <th>Tree 24</th>\n",
       "      <th>Tree 25</th>\n",
       "      <th>Tree 26</th>\n",
       "      <th>Tree 27</th>\n",
       "      <th>Tree 28</th>\n",
       "      <th>Tree 29</th>\n",
       "      <th>Tree 3</th>\n",
       "      <th>Tree 30</th>\n",
       "      <th>Tree 31</th>\n",
       "      <th>Tree 32</th>\n",
       "      <th>Tree 33</th>\n",
       "      <th>Tree 34</th>\n",
       "      <th>Tree 35</th>\n",
       "      <th>Tree 36</th>\n",
       "      <th>Tree 37</th>\n",
       "      <th>Tree 38</th>\n",
       "      <th>Tree 39</th>\n",
       "      <th>Tree 4</th>\n",
       "      <th>Tree 40</th>\n",
       "      <th>Tree 41</th>\n",
       "      <th>Tree 42</th>\n",
       "      <th>Tree 43</th>\n",
       "      <th>Tree 44</th>\n",
       "      <th>Tree 45</th>\n",
       "      <th>Tree 46</th>\n",
       "      <th>Tree 47</th>\n",
       "      <th>Tree 48</th>\n",
       "      <th>Tree 49</th>\n",
       "      <th>Tree 5</th>\n",
       "      <th>Tree 50</th>\n",
       "      <th>Tree 51</th>\n",
       "      <th>Tree 52</th>\n",
       "      <th>Tree 53</th>\n",
       "      <th>Tree 54</th>\n",
       "      <th>Tree 6</th>\n",
       "      <th>Tree 7</th>\n",
       "      <th>Tree 8</th>\n",
       "      <th>Tree 9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Tree 0  Tree 1  Tree 10  Tree 11  Tree 12  Tree 13  Tree 14  Tree 15  Tree 16  Tree 17  Tree 18  Tree 19  Tree 2  Tree 20  Tree 21  Tree 22  Tree 23  Tree 24  Tree 25  Tree 26  Tree 27  Tree 28  Tree 29  Tree 3  Tree 30  Tree 31  Tree 32  Tree 33  Tree 34  Tree 35  Tree 36  Tree 37  Tree 38  Tree 39  Tree 4  Tree 40  Tree 41  Tree 42  Tree 43  Tree 44  Tree 45  Tree 46  Tree 47  Tree 48  Tree 49  Tree 5  Tree 50  Tree 51  Tree 52  Tree 53  Tree 54  Tree 6  Tree 7  Tree 8  Tree 9\n",
       "0     0.0     1.0      1.0      1.0      1.0      1.0      1.0      0.0      1.0      1.0      1.0      1.0     1.0      1.0      1.0      1.0      1.0      1.0      1.0      1.0      0.0      1.0      1.0     1.0      1.0      1.0      1.0      1.0      1.0      1.0      0.0      1.0      1.0      1.0     1.0      1.0      1.0      1.0      1.0      1.0      1.0      1.0      1.0      1.0      1.0     1.0      1.0      0.0      1.0      1.0      1.0     1.0     1.0     1.0     1.0\n",
       "1     1.0     1.0      1.0      0.0      1.0      1.0      1.0      1.0      1.0      0.0      1.0      1.0     0.0      0.0      1.0      1.0      1.0      0.0      1.0      1.0      1.0      1.0      0.0     0.0      1.0      0.0      0.0      1.0      0.0      1.0      1.0      1.0      0.0      0.0     1.0      0.0      0.0      0.0      1.0      0.0      0.0      1.0      1.0      1.0      1.0     0.0      1.0      1.0      1.0      0.0      1.0     1.0     1.0     1.0     1.0\n",
       "2     1.0     1.0      1.0      1.0      1.0      1.0      1.0      1.0      1.0      0.0      1.0      1.0     0.0      1.0      1.0      1.0      1.0      1.0      1.0      0.0      0.0      1.0      1.0     1.0      0.0      0.0      1.0      1.0      1.0      1.0      0.0      1.0      1.0      1.0     1.0      1.0      0.0      1.0      1.0      1.0      1.0      1.0      1.0      1.0      1.0     1.0      0.0      0.0      1.0      1.0      1.0     1.0     1.0     0.0     1.0\n",
       "3     1.0     1.0      0.0      1.0      1.0      1.0      0.0      1.0      1.0      0.0      1.0      1.0     1.0      1.0      0.0      1.0      1.0      1.0      1.0      1.0      1.0      0.0      0.0     1.0      1.0      0.0      1.0      0.0      1.0      1.0      1.0      0.0      1.0      1.0     1.0      1.0      1.0      1.0      1.0      0.0      1.0      0.0      1.0      1.0      1.0     1.0      1.0      1.0      1.0      0.0      1.0     1.0     1.0     0.0     1.0\n",
       "4     0.0     0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0     0.0      0.0      1.0      0.0      0.0      0.0      1.0      1.0      0.0      0.0      0.0     0.0      0.0      1.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0     0.0      0.0      0.0      0.0      1.0      1.0      0.0      0.0      0.0      1.0      0.0     0.0      0.0      1.0      0.0      0.0      1.0     0.0     0.0     0.0     0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tree 0</th>\n",
       "      <th>Tree 1</th>\n",
       "      <th>Tree 10</th>\n",
       "      <th>Tree 11</th>\n",
       "      <th>Tree 12</th>\n",
       "      <th>Tree 13</th>\n",
       "      <th>Tree 14</th>\n",
       "      <th>Tree 15</th>\n",
       "      <th>Tree 16</th>\n",
       "      <th>Tree 17</th>\n",
       "      <th>Tree 18</th>\n",
       "      <th>Tree 19</th>\n",
       "      <th>Tree 2</th>\n",
       "      <th>Tree 20</th>\n",
       "      <th>Tree 21</th>\n",
       "      <th>Tree 22</th>\n",
       "      <th>Tree 23</th>\n",
       "      <th>Tree 24</th>\n",
       "      <th>Tree 25</th>\n",
       "      <th>Tree 26</th>\n",
       "      <th>Tree 27</th>\n",
       "      <th>Tree 28</th>\n",
       "      <th>Tree 29</th>\n",
       "      <th>Tree 3</th>\n",
       "      <th>Tree 30</th>\n",
       "      <th>Tree 31</th>\n",
       "      <th>Tree 32</th>\n",
       "      <th>Tree 33</th>\n",
       "      <th>Tree 34</th>\n",
       "      <th>Tree 35</th>\n",
       "      <th>Tree 36</th>\n",
       "      <th>Tree 37</th>\n",
       "      <th>Tree 38</th>\n",
       "      <th>Tree 39</th>\n",
       "      <th>Tree 4</th>\n",
       "      <th>Tree 40</th>\n",
       "      <th>Tree 41</th>\n",
       "      <th>Tree 42</th>\n",
       "      <th>Tree 43</th>\n",
       "      <th>Tree 44</th>\n",
       "      <th>Tree 45</th>\n",
       "      <th>Tree 46</th>\n",
       "      <th>Tree 47</th>\n",
       "      <th>Tree 48</th>\n",
       "      <th>Tree 49</th>\n",
       "      <th>Tree 5</th>\n",
       "      <th>Tree 50</th>\n",
       "      <th>Tree 51</th>\n",
       "      <th>Tree 52</th>\n",
       "      <th>Tree 53</th>\n",
       "      <th>Tree 54</th>\n",
       "      <th>Tree 6</th>\n",
       "      <th>Tree 7</th>\n",
       "      <th>Tree 8</th>\n",
       "      <th>Tree 9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Tree 0  Tree 1  Tree 10  Tree 11  Tree 12  Tree 13  Tree 14  Tree 15  Tree 16  Tree 17  Tree 18  Tree 19  Tree 2  Tree 20  Tree 21  Tree 22  Tree 23  Tree 24  Tree 25  Tree 26  Tree 27  Tree 28  Tree 29  Tree 3  Tree 30  Tree 31  Tree 32  Tree 33  Tree 34  Tree 35  Tree 36  Tree 37  Tree 38  Tree 39  Tree 4  Tree 40  Tree 41  Tree 42  Tree 43  Tree 44  Tree 45  Tree 46  Tree 47  Tree 48  Tree 49  Tree 5  Tree 50  Tree 51  Tree 52  Tree 53  Tree 54  Tree 6  Tree 7  Tree 8  Tree 9\n",
       "0     1.0     0.0      1.0      1.0      0.0      1.0      1.0      0.0      0.0      1.0      1.0      1.0     0.0      1.0      0.0      1.0      1.0      1.0      1.0      1.0      1.0      0.0      1.0     1.0      1.0      1.0      1.0      0.0      1.0      1.0      1.0      0.0      1.0      1.0     0.0      0.0      0.0      0.0      1.0      1.0      0.0      0.0      1.0      1.0      0.0     0.0      1.0      1.0      1.0      1.0      1.0     1.0     1.0     0.0     0.0\n",
       "1     0.0     1.0      1.0      0.0      0.0      0.0      1.0      0.0      0.0      0.0      0.0      1.0     0.0      0.0      1.0      0.0      1.0      0.0      0.0      1.0      0.0      1.0      1.0     1.0      1.0      0.0      1.0      1.0      0.0      1.0      1.0      1.0      1.0      0.0     1.0      1.0      1.0      0.0      1.0      1.0      0.0      0.0      1.0      0.0      1.0     1.0      1.0      1.0      1.0      0.0      0.0     1.0     1.0     1.0     1.0\n",
       "2     0.0     1.0      0.0      1.0      1.0      0.0      0.0      1.0      0.0      0.0      1.0      0.0     1.0      0.0      1.0      1.0      1.0      1.0      0.0      0.0      0.0      1.0      0.0     0.0      1.0      1.0      0.0      1.0      1.0      1.0      0.0      0.0      0.0      1.0     0.0      0.0      0.0      0.0      0.0      0.0      1.0      0.0      0.0      0.0      0.0     1.0      1.0      0.0      0.0      0.0      0.0     1.0     0.0     1.0     0.0\n",
       "3     0.0     1.0      1.0      1.0      1.0      0.0      0.0      1.0      0.0      1.0      1.0      0.0     1.0      1.0      1.0      1.0      1.0      1.0      0.0      1.0      1.0      1.0      1.0     1.0      1.0      1.0      1.0      0.0      1.0      0.0      1.0      1.0      0.0      1.0     1.0      1.0      1.0      1.0      1.0      0.0      1.0      1.0      1.0      1.0      1.0     0.0      1.0      0.0      0.0      1.0      1.0     0.0     1.0     1.0     0.0\n",
       "4     1.0     0.0      1.0      0.0      1.0      1.0      1.0      0.0      1.0      1.0      0.0      0.0     1.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0      1.0     1.0      0.0      1.0      0.0      1.0      0.0      0.0      0.0      0.0      0.0      0.0     0.0      1.0      0.0      0.0      0.0      1.0      0.0      1.0      1.0      0.0      0.0     0.0      0.0      1.0      0.0      1.0      1.0     0.0     1.0     0.0     0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_trees = 55 \n",
    "bagging_train_df, bagging_test_df, bagging_models = bagger(n_trees, tree_depth,  random_seed= 0)\n",
    "display(bagging_train_df.head())\n",
    "display(bagging_test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 2.2 Check that you have the requested function & variables\n",
    "assert 'bagger' in globals(), f\"Function 'bagger()' does not exist!\"\n",
    "for var in ['bagging_train_df', 'bagging_test_df']:\n",
    "    assert var in globals(), f\"Variable '{var}' does not exist!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3**  _Aggregate_ all 55 _bootstrapped_ models to get a combined prediction for each training and test point: predict a 1 if and only if a majority of the models predict that example to be from class 1. Assign the bagging accuracy test to a variable name `bagging_accuracy_test`. What accuracy does this *bagging* model achieve on the test set? Write an assertion that verifies that this test-set accuracy is at least as good as the accuracy for the model you fit in Question 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</span></div><div class='alert alert-block alert-danger'><b>WARNING!:</b><span style = 'color:black'> Do not delete any of the `### cs109Test() ###` comment lines!</span></div></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging train accuracy is 1.00\n",
      "Bagging test accuracy is 0.68\n"
     ]
    }
   ],
   "source": [
    "### cs109test(test_2.3) ### \n",
    "bagging_predictions_train = np.array([int(x) for x in np.mean(bagging_train_df, axis=1) > 0.5])\n",
    "bagging_accuracy_train = sum(bagging_predictions_train == y_train) / len(y_train)\n",
    "bagging_predictions_test = np.array([int(x) for x in np.mean(bagging_test_df, axis=1) > 0.5])\n",
    "bagging_accuracy_test = sum(bagging_predictions_test == y_test) / len(y_test)\n",
    "print(\"Bagging train accuracy is %.2f\" % bagging_accuracy_train)\n",
    "print(\"Bagging test accuracy is %.2f\" % bagging_accuracy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert 'bagging_accuracy_test' >= 'accuracy_score'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 2.3 Check that you have the requested variable\n",
    "assert 'bagging_accuracy_test' in globals(), f\"Variable 'bagging_accuracy_test' does not exist!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4** We want to know how the number of bootstraps affects our bagging ensemble's performance. Use the `running_predictions` function (given below) to get the model's accuracy score when using only 1,2,3,4,... of the bootstrapped models. Make a plot of training and test set accuracies as a function of number of bootstraps.\n",
    "** use the depth that you used above **\n",
    "\n",
    "On your plot, also include horizontal lines for two baselines:\n",
    "- the test accuracy of the best model from question 1\n",
    "- the test accuracy of a single tree with the tree depth you chose in 2.1, trained on the full training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def running_predictions(prediction_dataset: np.ndarray, targets: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"A function to predict examples' class via the majority among trees (ties are predicted as 0)\n",
    "    \n",
    "    Inputs:\n",
    "      prediction_dataset - a (n_examples by n_sub_models) dataset (not a dataframe), where each entry [i,j] is sub-model j's prediction\n",
    "          for example i\n",
    "      targets - the true class labels\n",
    "    \n",
    "    Returns:\n",
    "      a vector where vec[i] is the model's accuracy when using just the first i+1 sub-models\n",
    "    \"\"\"\n",
    "    \n",
    "    n_trees = prediction_dataset.shape[1]\n",
    "    \n",
    "    # find the running percentage of models voting 1 as more models are considered\n",
    "    running_percent_1s = np.cumsum(prediction_dataset, axis=1)/np.arange(1,n_trees+1)\n",
    "    \n",
    "    # predict 1 when the running average is above 0.5\n",
    "    running_conclusions = running_percent_1s > 0.5\n",
    "    \n",
    "    # check whether the running predictions match the targets\n",
    "    running_correctnesss = running_conclusions == targets.reshape(-1,1)\n",
    "    \n",
    "    return np.mean(running_correctnesss, axis=0)\n",
    "    # returns a 1-d series of the accuracy of using the first n trees to predict the targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot broadcast shape [(5000, 55)] with block values [(5000, 1)]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-6fb61c06147e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# plotting code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrunning_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunning_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbagging_test_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m56\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Bagging on resampled data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m55\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtest_score\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Single tree, overfit\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m55\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbest_cv_tree_test_score\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Single tree, best fit\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-d3ab63a25f02>\u001b[0m in \u001b[0;36mrunning_predictions\u001b[0;34m(prediction_dataset, targets)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# check whether the running predictions match the targets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mrunning_correctnesss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunning_conclusions\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunning_correctnesss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   1302\u001b[0m             \u001b[0;31m# straight boolean comparisions we want to allow all columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m             \u001b[0;31m# (regardless of dtype to pass thru) See #4537 for discussion.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_combine_const\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_on_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1305\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_combine_const\u001b[0;34m(self, other, func, raise_on_error)\u001b[0m\n\u001b[1;32m   3541\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_combine_const\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_on_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3542\u001b[0m         new_data = self._data.eval(func=func, other=other,\n\u001b[0;32m-> 3543\u001b[0;31m                                    raise_on_error=raise_on_error)\n\u001b[0m\u001b[1;32m   3544\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   3195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3197\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'eval'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mquantile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, axes, filter, do_integrity_check, consolidate, **kwargs)\u001b[0m\n\u001b[1;32m   3089\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3090\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mgr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3091\u001b[0;31m             \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3092\u001b[0m             \u001b[0mresult_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3093\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, func, other, raise_on_error, try_cast, mgr)\u001b[0m\n\u001b[1;32m   1131\u001b[0m                     raise ValueError(\"cannot broadcast shape [%s] with block \"\n\u001b[1;32m   1132\u001b[0m                                      \"values [%s]\" % (values.T.shape,\n\u001b[0;32m-> 1133\u001b[0;31m                                                       other.shape))\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0mtransf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_transposed\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot broadcast shape [(5000, 55)] with block values [(5000, 1)]"
     ]
    }
   ],
   "source": [
    "# plotting code\n",
    "running_accuracy = running_predictions(bagging_test_df, y_test)\n",
    "plt.plot(range(1, 56), running_accuracy, label=\"Bagging on resampled data\")\n",
    "plt.plot([1, 55], [test_score]*2, label=\"Single tree, overfit\")\n",
    "plt.plot([1, 55], [best_cv_tree_test_score]*2, label=\"Single tree, best fit\")\n",
    "plt.xlabel(\"Number of trees\")\n",
    "plt.ylabel(\"Accuracy score\")\n",
    "plt.title(\"Running accuracy by number of trees\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 2.4 Check that you have the requested function\n",
    "assert 'running_predictions' in globals(), f\"Function 'running_predictions()' does not exist!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.5** Referring to your graph from 2.4, compare the performance of bagging against the baseline of a single `tree_depth` tree. Explain the differences you see.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes only 3 trees trained on resampled data to outperform the single `tree_depth` tree trained on the entire dataset. This is because the depth we selected intentionally overfits the data. Thus, each individual tree can be considered a strong (unconstrained) learner with poor generalization, but bagging allows us to turn many such models into one combined model that improves on generalization while retaining predictive ability. \n",
    "\n",
    "Although it takes 9 trees to outperform the best CV tree model, we see significant and sustained outperformance for a high-N bagging model versus a single well-fit decision tree. This is because bagging provides a superior way of capturing the complexity of the data, through our individual high-variance learners, while limiting variance overall, by averaging their predictions and thus mitigating the impact of the noise in each one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.6** Bagging and limiting tree depth both affect how much the model overfits. Compare and contrast these two approaches. Your answer should refer to your graph in 2.4 and may duplicate something you said in your answer to 1.5.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limiting tree depth prevents overfitting by providing an upper bound on the complexity of the model, much like regularization penalizes extreme coefficients or PCA with a well-chosen number of axes achieves good performance with significant dimensionality reduction. However, the downside is that even legitimate and predictive sources of variation may be dismissed if they do not have a strong enough impact on the result.\n",
    "\n",
    "Bagging averages over the error in individual models, which are usually poor generalizers on an individual level, to provide an overall prediction that keeps legitimate predictors (because their impact will be similar in almost every model) while mitigating the impact of random noise (which should affect models in different directions, with the mean impact approaching zero as the number of models rises). This method can provide superior generalization, though at the cost of additional and somewhat redundant training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.7**: In what ways might our bagging classifier be overfitting the data? In what ways might it be underfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our bagging classifier becomes more effective the more trees it uses, due mainly to improvements in reducing overfitting. At low N, bagging is only somewhat effective, because each tree strongly overfits the bootstrapped data it was trained on and underfits any data it has not yet seen. The combined model suffers from this random pattern of overfitting and underfitting at first, but the effect of missing or overrepresented data points is mitigated as we add more models whose random elements tend to cancel each other's effects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'> <b> Question 3 [20 pts]: Random Forests </b> </div>\n",
    "Random Forests are closely related to the bagging model we built by hand in question 2. In this question we compare our by-hand results with the results of using `RandomForestClassifier` directly.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**3.1**  Fit a `RandomForestClassifier` to the original `X_train` data using the same tree depth and number of trees that you used in Question 2.2. Use number of features to consider when looking for the best split to be the `sqrt(total_number of features`. Evaluate its accuracy on the test set and assign it to a variable name `random_forest_test_score`.\n",
    "\n",
    "**3.2** Among all of the decision trees you fit in the bagging process, how many times is each feature used as the top/first node? How about for each tree in the random forest you just fit? What about the process of training the Random Forest causes this difference? What implication does this observation have on the accuracy of bagging vs Random Forest?\n",
    "Assign this to two pandas Series called `top_predictors_bagging` and `top_predictors_rf` and give them an example] \n",
    "\n",
    "**Hint**: A decision tree's top feature is stored in `model.tree_.feature[0]`. A random forest object stores its decision trees in its `.estimators_` attribute.\n",
    "\n",
    "\n",
    "**3.3**: Make a pandas table of the training and test accuracy for the following models and name it `results_df`:\n",
    "\n",
    "- Single tree with best depth chosen by cross-validation (from Question 1)\n",
    "- A single overfit tree trained on all data (from Question 2, using the depth you chose there)\n",
    "- Bagging 55 such trees (from Question 2)\n",
    "- A Random Forest of 55 such trees (from Question 3.1)\n",
    "\n",
    "(see below for the expected structure)  \n",
    "\n",
    "(This problem should not require fitting any new models, though you may need to go back and store the accuracies from models you fit previously.)\n",
    "\n",
    "What is the relative performance of each model on the training set? On the test set? Comment on how these relationships make sense (or don't make sense) in light of how each model treats the bias-variance trade-off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1**  Fit a `RandomForestClassifier` to the original `X_train` data using the same tree depth and number of trees that you used in Question 2.2. Use number of features to consider when looking for the best split to be the `sqrt(total_number of features`. Evaluate its accuracy on the test set and assign it to a variable name `random_forest_test_score`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</span></div><div class='alert alert-block alert-danger'><b>WARNING!:</b><span style = 'color:black'> Do not delete any of the `### cs109Test() ###` comment lines!</span></div></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### cs109test(test_3.1) ### \n",
    "random_forest = RandomForestClassifier(n_estimators=n_trees, max_features='sqrt').fit(X_train, y_train)\n",
    "random_forest_train_score = random_forest.score(X_train, y_train)\n",
    "random_forest_test_score = random_forest.score(X_test, y_test)\n",
    "print('Random forest train score is %.2f' % random_forest_train_score)\n",
    "print('Random forest test score is %.2f' % random_forest_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 3.1 Check that you have the requested variable\n",
    "assert 'random_forest_test_score' in globals(), f\"Variable 'random_forest_test_score' does not exist!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2** Among all of the decision trees you fit in the bagging process, how many times is each feature used as the top/first node? How about for each tree in the random forest you just fit? What about the process of training the Random Forest causes this difference? What implication does this observation have on the accuracy of bagging vs Random Forest?\n",
    "Assign this to two pandas Series called `top_predictors_bagging` and `top_predictors_rf` and give them an example]\n",
    "\n",
    "**Hint**: A decision tree's top feature is stored in `model.tree_.feature[0]`. A random forest object stores its decision trees in its `.estimators_` attribute.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</span></div><div class='alert alert-block alert-danger'><b>WARNING!:</b><span style = 'color:black'> Do not delete any of the `### cs109Test() ###` comment lines!</span></div></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### cs109test(test_3.2) ### \n",
    "from collections import Counter\n",
    "first_features_rf = Counter([model.tree_.feature[0] for model in random_forest.estimators_])\n",
    "first_features_bag = Counter([model.tree_.feature[0] for model in bagging_models])\n",
    "top_predictors_rf = pd.Series(first_features_rf.keys())\n",
    "top_predictors_bagging = pd.Series(first_features_bag.keys())\n",
    "print(\"Random forest: %s\" % str(first_features_rf))\n",
    "print(\"Bagging: %s\" % str(first_features_bag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest prioritizes different features in different subtrees, whereas our bagging approach prioritizes the same feature every time (though perhaps to a diffrent extent). This result is explained by the fact that we set no upper limit on the number of features used in the bagging models, whereas `max_features` for our random forest is set to the square root of the total number of features.\n",
    "\n",
    "Essentially, we are forcing the random forest to vary more in its training, which may further mitigate our overfitting problem by preventing <i>repeated</i> overfitting that occurs in the same direction for most or all individual trees. Thus, although each subtree in the random forest may be slightly less accurate on its training data, we should expect slightly superior accuracy for the random forest overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 3.2 Check that you have the requested variables\n",
    "for var in ['top_predictors_bagging', 'top_predictors_rf']:\n",
    "    assert var in globals(), f\"Variable '{var}' does not exist!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3**: Make a pandas table of the training and test accuracy for the following models and name it `results_df`:\n",
    "\n",
    "- Single tree with best depth chosen by cross-validation (from Question 1)\n",
    "- A single overfit tree trained on all data (from Question 2, using the depth you chose there)\n",
    "- Bagging 55 such trees (from Question 2)\n",
    "- A Random Forest of 55 such trees (from Question 3.1)\n",
    "\n",
    "(see below for the expected structure)\n",
    "\n",
    "(This problem should not require fitting any new models, though you may need to go back and store the accuracies from models you fit previously.)\n",
    "\n",
    "What is the relative performance of each model on the training set? On the test set? Comment on how these relationships make sense (or don't make sense) in light of how each model treats the bias-variance trade-off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the following table.\n",
    "\n",
    "\n",
    "| classifier | training accuracy | test accuracy |\n",
    "| --- | --- | --- |\n",
    "| single tree with best depth chosen by CV | | |\n",
    "| single depth-X tree | | |\n",
    "| bagging 55 depth-X trees | | |\n",
    "| Random Forest of 55 depth-X trees | | |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = {'classifier': ['single tree with best depth chosen by CV',\n",
    "                       'single depth-20 tree',\n",
    "                       'bagging 55 depth-20 trees',\n",
    "                       'Random Forest of 55 depth-20 trees'],\n",
    "        'training accuracy': [best_cv_tree_train_score,\n",
    "                              train_score,\n",
    "                              bagging_accuracy_train,\n",
    "                              random_forest_train_score],\n",
    "        'test accuracy': [best_cv_tree_test_score,\n",
    "                          test_score,\n",
    "                          bagging_accuracy_test,\n",
    "                          random_forest_test_score],\n",
    "       }\n",
    "results_df = pd.DataFrame(cols)\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The single limited-depth tree only achieves 68% accuracy on training, while the other models are nearly perfect, which makes sense in light of the fact that the single depth-20 tree and the bagging models all have at least the necessary complexity to \"remember\" every data point in the training set.\n",
    "\n",
    "In terms of performance, we have the following ranking:\n",
    "\n",
    "1. Random forest\n",
    "\n",
    "2. Bagging\n",
    "\n",
    "3. Best single tree\n",
    "\n",
    "4. Deep single tree\n",
    "\n",
    "This makes sense in that each model addresses a deficiency of the one below it: the single tree reduces the overfitting of the deep tree, bagging captures the data more completely than the single tree, and the random forest with a limited feature count encourages diversification between individual trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 3.3 Check that you have the requested variable\n",
    "assert 'results_df' in globals(), \"Variable 'results_df' does not exist!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'><b> Question 4 [15 pts]: Boosting </div>\n",
    "In this question we explore a different kind of ensemble method, boosting, where each new model is trained on a dataset weighted towards observations that the current set of models predicts incorrectly. \n",
    "\n",
    "We'll focus on the AdaBoost flavor of boosting and examine what happens to the ensemble model's accuracy as the algorithm adds more estimators (iterations) to the ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.1** We'll motivate AdaBoost by noticing patterns in the errors that a single classifier makes. Fit `tree1`, a decision tree with depth 3, to the training data. \n",
    "Report the train and test accuracies. For each predictor, make a plot that compares two distributions: the values of that predictor for examples that `tree1` classifies correctly, and the values of that predictor for examples that `tree1` classifies incorrectly. Do you notice any predictors for which the distributions are clearly different?\n",
    "\n",
    "*Hints*:\n",
    "- If you have `fig, axs = plt.subplots(...)`, then `axs.ravel()` gives a list of each plot in reading order.\n",
    "- [`sns.kdeplot`](https://seaborn.pydata.org/generated/seaborn.kdeplot.html) takes `ax` and `label` parameters.\n",
    "\n",
    "**4.2** The following code (see below) attempts to implement a simplified version of boosting using just two classifiers (described below). However, it has both stylistic and functionality flaws. First, imagine that you are a grader for a Data Science class; write a comment for the student who submitted this code. Then, imagine that you're the TF writing the solutions; make an excellent example implementation. Finally, use your corrected code to compare the performance of `tree1` and the boosted algorithm on both the training and test set.\n",
    "\n",
    "**4.3** Now let's use the sklearn implementation of AdaBoost: Use `AdaBoostClassifier` to fit another ensemble to `X_train`. Use a decision tree of depth 3 as the base learner and a learning rate 0.05, and run the boosting for 800 iterations. Make a plot of the effect of the number of estimators/iterations on the model's train and test accuracy.\n",
    "\n",
    "*Hint*: The `staged_score` method provides the accuracy numbers you'll need. You'll need to use `list()` to convert the \"generator\" it returns into an ordinary list.\n",
    "\n",
    "**4.4** Repeat the plot above for a base learner with depth of (1, 2, 3, 4). What trends do you see in the training and test accuracy?\n",
    "\n",
    "(It's okay if your code re-fits the depth-3 classifier instead of reusing the results from the previous problem.)\n",
    "\n",
    "**4.5** Based on the plot you just made, what combination of base learner depth and number of iterations seems optimal? Why? How does the performance of this model compare with the performance of the ensembles you considered above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.1** We'll motivate AdaBoost by noticing patterns in the errors that a single classifier makes. Fit `tree1`, a decision tree with depth 3, to the training data.\n",
    "Report the train and test accuracies. For each predictor, make a plot that compares two distributions: the values of that predictor for examples that `tree1` classifies correctly, and the values of that predictor for examples that `tree1` classifies incorrectly. Do you notice any predictors for which the distributions are clearly different?\n",
    "\n",
    "*Hints*:\n",
    "- If you have `fig, axs = plt.subplots(...)`, then `axs.ravel()` gives a list of each plot in reading order.\n",
    "- [`sns.kdeplot`](https://seaborn.pydata.org/generated/seaborn.kdeplot.html) takes `ax` and `label` parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree1 = DecisionTreeClassifier(max_depth=3).fit(X_train, y_train)\n",
    "tree1_train_score, tree1_test_score = tree1.score(X_train, y_train), tree1.score(X_test, y_test)\n",
    "print(\"Depth 3 train, test accuracy: %.2f, %.2f\" % (tree1_train_score, tree1_test_score))\n",
    "n_features = len(X_train[0])\n",
    "fig, axs = plt.subplots(n_features, 2, figsize=(16, 80))\n",
    "for i, ax in enumerate(axs.ravel()):\n",
    "    col = i // 2\n",
    "    side = i % 2\n",
    "    X_right = X_train[:, col][[round(p) for p in tree1.predict(X_train)] == y_train]\n",
    "    X_wrong = X_train[:, col][[round(p) for p in tree1.predict(X_train)] != y_train] \n",
    "    sns.kdeplot(X_wrong if side else X_right, ax=ax,\n",
    "                label='Feature %d, %s' % (col, 'incorrect' if side else 'correct'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature 25 clearly has a wider distribution on the incorrect cases than on the correct ones, and Feature 18 has an uneven density plot for the incorrect cases whereas the correct cases are much more symmetric. However, beyond a general tendency for the incorrect cases to be slightly more widely distributed, the shapes of the feature distributions are quite similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 4.1 Check that you have the requested variable\n",
    "assert 'tree1' in globals(), \"Variable 'tree1' does not exist!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.2** The following code (see below) attempts to implement a simplified version of boosting using just two classifiers (described below). However, it has both stylistic and functionality flaws. First, imagine that you are a grader for a Data Science class; write a comment for the student who submitted this code. Then, imagine that you're the TF writing the solutions; make an excellent example implementation. Finally, use your corrected code to compare the performance of `tree1` and the boosted algorithm on both the training and test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intended functionality is the following:\n",
    "1. Fit `tree1`, a decision tree with max depth 3.\n",
    "2. Construct an array of sample weights. Give a weight of 1 to samples that `tree1` classified correctly, and 2 to samples that `tree1` misclassified.\n",
    "3. Fit `tree2`, another depth-3 decision tree, using those sample weights.\n",
    "4. To predict, compute the probabilities that `tree1` and `tree2` each assign to the positive class. Take the average of those two probabilities as the prediction probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def boostmeup(X, y):\n",
    "    tree = DecisionTreeClassifier(max_depth=3)\n",
    "    tree1 = tree.fit(X, y)\n",
    "    sample_weight = np.ones(len(X_train))\n",
    "    q = 0\n",
    "    for idx in range(len(X_train)):\n",
    "          if tree1.predict([X_train[idx]]) != y_train[idx]:\n",
    "             sample_weight[idx] = sample_weight[idx] * 2\n",
    "             q = q + 1\n",
    "    print(\"tree1 accuracy:\", q / len(X_train))\n",
    "    tree2 = tree.fit(X_train, y_train, sample_weight=sample_weight)\n",
    "    \n",
    "# Train\n",
    "    q = 0\n",
    "    for idx in range(len(X_train)):\n",
    "        t1p = tree1.predict_proba([X_train[idx]])[0][1]\n",
    "        t2p = tree2.predict_proba([X_train[idx]])[0][1]\n",
    "        m = (t1p + t2p) / 2\n",
    "        if m > .5:\n",
    "            if y_train[idx] == True:\n",
    "                q = q + 0\n",
    "            else:\n",
    "                q = q + 1\n",
    "        else:\n",
    "            if y_train[idx] == True:\n",
    "                q = q + 1\n",
    "            else:\n",
    "                q = 0\n",
    "    print(\"Boosted accuracy:\", q / len(X_train))\n",
    "\n",
    "# Test\n",
    "    q = 0\n",
    "    for idx in range(len(X_test)):\n",
    "        t1p = tree1.predict_proba([X_test[idx]])[0][1]\n",
    "        t2p = tree2.predict_proba([X_test[idx]])[0][1]\n",
    "        m = (t1p + t2p) / 2\n",
    "        if m > .5:\n",
    "            if y_train[idx] == True:\n",
    "                q = q + 0\n",
    "            else:\n",
    "                q = q + 1\n",
    "        else:\n",
    "            if y_train[idx] == True:\n",
    "                q = q + 1\n",
    "            else:\n",
    "                q = 0\n",
    "    print(\"Boosted accuracy:\", q / len(X_test))\n",
    "\n",
    "boostmeup(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functionality:\n",
    "\n",
    "- The critical failure is that tree2 and tree1 refer to the same object, so the \"boosted\" ensemble is actually two instances of one retrained tree.\n",
    "- The printed tree1 accuracy is actually the fraction of misclassified samples, not correctly classified samples.\n",
    "- The boosted accuracies reported are also misclassification rates, not accuracy scores.\n",
    "- The X and y arguments are ignored midway through in favor of X_train and y_train, so the function would not work at all if X_train and y_train changed.\n",
    "\n",
    "Style:\n",
    "\n",
    "- This code should be vectorized.\n",
    "- The print statement is unnecessary within the function and should be handled separately.\n",
    "- Max depth should be an argument to the function.\n",
    "- This code is quite repetitive and could easily be shortened to about a third of its length. Extracting the common code between the train and test cases into a function that takes a data matrix and a list of targets would accomplish most of the process, and that code could be further condensed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_boost(t1, t2, X, y, label='train'):\n",
    "    predictions = np.round((t1.predict_proba(X)[:, 1] + t2.predict_proba(X)[:, 1]) / 2)\n",
    "    print('Boosted accuracy for %s: %.4f' % (label, sum(predictions == y) / len(y)))\n",
    "\n",
    "def simple_boost(X, y, depth):\n",
    "    tree1 = DecisionTreeClassifier(max_depth=depth).fit(X, y)\n",
    "    weight = np.ones(len(X)) + np.ones(len(X)) * (np.round(tree1.predict(X)) != y)\n",
    "    tree2 = DecisionTreeClassifier(max_depth=depth).fit(X, y, sample_weight=weight)\n",
    "    return tree1, tree2\n",
    "\n",
    "tree1, tree2 = simple_boost(X_train, y_train, 3)\n",
    "print(\"Tree1 accuracy for train: %.4f\" % tree1.score(X_train, y_train))\n",
    "print(\"Tree1 accuracy for test: %.4f\" % tree1.score(X_test, y_test))\n",
    "predict_boost(tree1, tree2, X_train, y_train, 'train')\n",
    "predict_boost(tree1, tree2, X_test, y_test, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 4.2 Check that you have the requested variables\n",
    "for var in ['tree1', 'tree2']:\n",
    "    assert var in globals(), f\"Variable '{var}' does not exist!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.3** Now let's use the sklearn implementation of AdaBoost: Use `AdaBoostClassifier` to fit another ensemble to `X_train`. Use a decision tree of depth 3 as the base learner and a learning rate 0.05, and run the boosting for 800 iterations. Make a plot of the effect of the number of estimators/iterations on the model's train and test accuracy.\n",
    "\n",
    "*Hint*: The `staged_score` method provides the accuracy numbers you'll need. You'll need to use `list()` to convert the \"generator\" it returns into an ordinary list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adaboost = AdaBoostClassifier(DecisionTreeClassifier(max_depth=3),\n",
    "                              learning_rate=0.05,\n",
    "                              n_estimators=800).fit(X_train, y_train)\n",
    "scores = list(adaboost.staged_score(X_test, y_test))\n",
    "plt.plot(range(1, 801), scores)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Boosting staged accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.4** Repeat the plot above for a base learner with depth of (1, 2, 3, 4). What trends do you see in the training and test accuracy?\n",
    "\n",
    "(It's okay if your code re-fits the depth-3 classifier instead of reusing the results from the previous problem.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_adaboost(X, y, depth, est):\n",
    "    return AdaBoostClassifier(DecisionTreeClassifier(max_depth=depth),\n",
    "                              learning_rate=0.05,\n",
    "                              n_estimators=est).fit(X, y)\n",
    "\n",
    "n_est = 800\n",
    "models = {d: get_adaboost(X_train, y_train, d, n_est) for d in range(1, 5)}\n",
    "scores = {d: list(models[d].staged_score(X_train, y_train)) for d in models.keys()}\n",
    "for d, staged in scores.items():\n",
    "    plt.plot(range(1, n_est+1), staged, label='Depth %d' % d)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Boosting staged accuracy (train)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = {d: list(models[d].staged_score(X_test, y_test)) for d in models.keys()}\n",
    "for d, staged in scores.items():\n",
    "    plt.plot(range(1, n_est+1), staged, label='Depth %d' % d)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Boosting staged accuracy (test)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training accuracy continues to improve as the number of iterations increases, but the decline in test accuracy shows that this method is prone to overfitting past a certain point. Also, higher depth is always better from a training perspective, but a middle ground is best for generalization (depth 3 in this case). \n",
    "\n",
    "Additionally, the more iterations we run, the more our higher-complexity boosted models begin to suffer from overfitting relative to their simpler counterparts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.5** Based on the plot you just made, what combination of base learner depth and number of iterations seems optimal? Why? How does the performance of this model compare with the performance of the ensembles you considered above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_ada = get_adaboost(X_train, y_train, 3, 100)\n",
    "print('Train: %.2f' % best_ada.score(X_train, y_train))\n",
    "print('Test: %.2f' % best_ada.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks as though depth 3 with some number of iterations between 40 and 100 performs best on the test set. This represents a configuration with enough expressive power to model the data (depth 3 is better than depth 1 or 2 in this case) but few enough iterations to avoid overfitting. The resulting model has relatively modest training accuracy, but achieves close to the global maximum on test accuracy, meaning that we have generalized about as well as we can under this regime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'> <b> Question 5 [15 pts]: Understanding </b></div>\n",
    "This question is an overall test of your knowledge of this homework's material. You may need to refer to lecture notes and other material outside this homework to answer these questions.\n",
    "\n",
    "\n",
    "\n",
    "**5.1** How do boosting and bagging relate: what is common to both, and what is unique to each?\n",
    "\n",
    "\n",
    "**5.2** Reflect on the overall performance of all of the different classifiers you have seen throughout this assignment. Which performed best? Why do you think that may have happened?\n",
    "\n",
    "**5.3** What is the impact of having too many trees in boosting and in bagging? In which instance is it worse to have too many trees?\n",
    "\n",
    "**5.4** Which technique, boosting or bagging, is better suited to parallelization, where you could have multiple computers working on a problem at the same time?\n",
    "\n",
    "**5.5** Which of these techniques can be extended to regression tasks? How?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.1** How do boosting and bagging relate: what is common to both, and what is unique to each?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both boosting and bagging are ensemble methods that combine many models in an attempt to mitigate the deficiencies of their individual components, producing a better and more generalizable model overall. However, these approaches come at the problem from opposite directions.\n",
    "\n",
    "Bagging is a parallel algorithm that combines strong learners - unconstrained models that tend to overfit the data - and averages their results to prevent overfitting and lower overall variance as the noise in individual predictions cancels out. The combined model tends to overfit some (most) data points and underfit others at the start, but improves as the number of models increases.\n",
    "\n",
    "Boosting is a serial algorithm that combines weak learners - highly constrained models unable to fully capture the structure of the data - and concentrates each successive model on the mispredictions of the existing ensemble. This method works well when the base model and number of iterations are well coordinated, but is prone to underfitting if the base model is too simple, or overfitting if the number of iterations is too high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.2** Reflect on the overall performance of all of the different classifiers you have seen throughout this assignment. Which performed best? Why do you think that may have happened?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost and RandomForest performed best, with a slight edge in favor of AdaBoost that is most likely explained by the fact that the assignment spent more time optimizing hyperparameters in the boosting scenario. Overall, these top two solutions attained similar performance, representing the convergence towards an optimal balance of complexity and generalizability. Although the two approach this optimum from opposite directions, RandomForest correcting for initial overfitting and AdaBoost for initial underfitting, each arrived at a similar level of predictive ability in the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.3** What is the impact of having too many trees in boosting and in bagging? In which instance is it worse to have too many trees?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging takes longer to train and to evaluate when the number of trees is too high, but performance does not degrade significantly because new trees essentially duplicate old ones; the main issue is diminishing returns. Too many trees in boosting causes a much more serious problem, as the combined model begins to overfit the training data, much as if we had instead trained a single tree of unlimited depth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.4** Which technique, boosting or bagging, is better suited to parallelization, where you could have multiple computers working on a problem at the same time?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging is much better for parallelization, in both training and evaluation, because each decision tree can be trained and run independently of the others. With boosting, each new model must have access to the output of the fully trained ensemble up to that point, so the training process cannot be parallelized. (Evaluation, however, could be.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.5** Which of these techniques can be extended to regression tasks? How?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both can be extended to regression by using regressors rather than classifiers as the base model.\n",
    "\n",
    "For bagging, the outputs of many base regressors can be integrated through either voting or averaging. This can be accomplished in Python with sklearn.ensemble.BaggingRegressor. Alternatively, if the base model is a decision tree, then decision trees whose \"classes\" are simply real numbers can be trained on subsets of the data and their outputs similarly combined. This functionality is implemented in sklearn.ensemble.RandomForestRegressor. \n",
    "\n",
    "For boosting, the weights for the marginal regressor can be set according to the numerical error, rather than the misclassification status, of the current ensemble's predictions. This can be done using sklearn.ensemble.AdaBoostRegressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "jupytext": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
